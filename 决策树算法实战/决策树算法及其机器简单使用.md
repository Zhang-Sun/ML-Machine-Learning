# 2.决策树

## 2.1 决策树简介

决策树(decision tree)是一种基本的分类与回归方法。如下图所示的流程图就可以理解为一个简单的决策树模型，长方形代表判断模块（desicion block）。椭圆形代表终止模块（terminating block），表示已经得出结论，可以终止运行。从判断模块引出的左右箭头称作为分支（branch），它可以达到另一个判断模块或者终止模块。我们可以这样理解，分类决策树模型是一种描述对实例进行分类的树形结构。决策树由节点（node）和有向边（derected edge）组成。节点有两种类型：内部节点（internal node）和叶子结点（leaf node）。内部节点表示一个特征或属性，叶子结点表示一个类。如下图所示的决策树模型，长方形和椭圆形都是节点。长方形是内部节点，椭圆形是叶子结点。从结点引出的左右箭头就是有向边。而最上面的结点就是决策树的根结点（root node）。可以这么理解，决策树也是树结构，因此具有树结构的一般性质。

![image](https://github.com/Zhang-Sun/My-first-proj/blob/机器学习/决策树算法实战/决策树图像与数据/20170721162306492.png
)

这是一个假象的相亲对象分类系统。它首先检测对方是否有房。如果有房，则这个相亲对象可以考虑进一步接触。如果没房，则观察相亲对象是否有上进心，如果没有，直接say goodbye，如果有上进心可以列为备胎。

这只是一个简单的分类系统，实施情况可能更加复杂。我们可以吧决策树看成一个if-then规则的集合，将决策树转换成if-then规则的过程是这样的：由决策树的根结点（root node）到叶子结点（leaf node）的每一条路径构建一条规则：路径上内部节点对应着规则的条件，而叶子结点对应着规则的结论。决策树的路径或其对应的if-then规则集合具有一个重要的性质：互斥并且完备。这就是说，每一个实例都被一条路径或一条规则所覆盖，而且只被一条路径或一条规则所覆盖。这里所覆盖是指实例的特征与路径的特征一致或满足规则的条件。

使用决策树预测需要以下过程：
* 收集数据：可以使用任何方法。比如构建一个相亲系统，我们可以从媒婆或者走访相亲对象来获取对方的数据。根据他们考虑的因素和最终选择的结果，就可以得到一些供我们利用的数据了。
* 准备数据：收集完数据，，我们要进行整理，将这些所有收集的数据按照一定规则整理出来，并且排版， 方便我们后续的处理。
* 分析数据：可以使用任何方法，决策树构造完成后，我们可以检查决策树图形是否符合预期。
* 训练算法：这个过程也就是构造决策树，同样也可以说是决策树学习，就是构造一个决策树的数据结构。
* 测试算法：使用经验树计算错误率。当错误率达到了可接受的范围，决策树就可以投入使用了。
* 使用算法：此步骤可以适用于所有监督学习算法，而使用决策树可以更好的理解数据的内在含义。

## 这里介绍一下监督学习和无监督学习：



### 监督学习（supervised learning）

从给定的训练数据集中学习出一个函数（模型参数），当新的数据到来时，可以根据这个函数预测结果。监督学习的训练集要求包括输入输出，也可以说是特征和目标。训练集中的目标是由人标注的。监督学习就是最常见的分类（注意和聚类区分）问题，通过已有的训练样本（即已知数据及其对应的输出）去训练得到一个最优模型（这个模型属于某个函数的集合，最优表示某个评价准则下是最佳的），再利用这个模型将所有的输入映射为相应的输出，对输出进行简单的判断从而实现分类的目的。也就具有了对未知数据分类的能力。监督学习的目标往往是让计算机去学习我们已经创建好的分类系统（模型）。

从给定的训练数据集中学习出一个函数（模型参数），当新的数据到来时，可以根据这个函数预测结果。监督学习的训练集要求包括输入输出，也可以说是特征和目标。训练集中的目标是由人标注的。监督学习就是最常见的分类（注意和聚类区分）问题，通过已有的训练样本（即已知数据及其对应的输出）去训练得到一个最优模型（这个模型属于某个函数的集合，最优表示某个评价准则下是最佳的），再利用这个模型将所有的输入映射为相应的输出，对输出进行简单的判断从而实现分类的目的。也就具有了对未知数据分类的能力。监督学习的目标往往是让计算机去学习我们已经创建好的分类系统（模型）。

常见的有监督学习算法：回归分析和统计分类。最典型的算法是KNN和SVM。

### 无监督学习（unsupervised learning）

输入数据没有被标记，也没有确定的结果。样本数据类别未知，需要根据样本间的相似性对样本集进行分类（聚类，clustering）试图使类内差距最小化，类间差距最大化。通俗点将就是实际应用中，不少情况下无法预先知道样本的标签，也就是说没有训练样本对应的类别，因而只能从原先没有样本标签的样本集开始学习分类器设计。

非监督学习目标不是告诉计算机怎么做，而是让它（计算机）自己去学习怎样做事情。非监督学习有两种思路。第一种思路是在指导Agent时不为其指定明确分类，而是在成功时，采用某种形式的激励制度。需要注意的是，这类训练通常会置于决策问题的框架里，因为它的目标不是为了产生一个分类系统，而是做出最大回报的决定，这种思路很好的概括了现实世界，agent可以对正确的行为做出激励，而对错误行为做出惩罚。

无监督学习的方法分为两大类：

1. 一类为基于概率密度函数估计的直接方法：指设法找到各类别在特征空间的分布参数，再进行分类。

2. 另一类是称为基于样本间相似性度量的简洁聚类方法：其原理是设法定出不同类别的核心或初始内核，然后依据样本与核心之间的相似性度量将样本聚集成不同的类别。

利用聚类结果，可以提取数据集中隐藏信息，对未来数据进行分类和预测。应用于数据挖掘，模式识别，图像处理等。

PCA和很多deep learning算法都属于无监督学习。

## 2.2 决策树构建的准备工作

使用决策树做预测每一步都很重要，数据收集不到位将会导致没有足够的特征够我们构建错误率低的决策树。数据特征充足，但是不知道用哪些特征好，将会导致无法构建出分类效果好的决策树模型。从算法方面看，决策树的构建是我们的核心内容。

决策树的构建过程通常分为三步：特征选择、决策树生成和决策树的修剪。

### 2.2.1 特征选择

特征选择在于选取对训练数据具有分类能力的特征。这样可以提高决策树的学习效率，如果利用一个特征进行分类的结果与随机分类的结果没有太大差别，那么这个特征就是没有分类能力的。经验上扔掉这些特征对决策树的学习精度影响不大。通常特征选择的标准是信息增益（information gain）或信息增益比，为了简单，本文使用信息增益作为选择特征的标准。在介绍信息增益之前让我们看一组数据，贷款申请样本数据表。

<table>
    <tr>
        <td>ID</td>
        <td>年龄</td>
        <td>有工作</td>
        <td>有自己的房子</td>
        <td>信贷情况</td>
        <td>类别(是否个给贷款)</td>
    </tr>
    <tr>
        <td>1</td>
        <td>青年</td>
        <td>否</td>
        <td>否</td>
        <td>一般</td>
        <td>否</td>
    </tr>
    <tr>
        <td>2</td>
        <td>青年</td>
        <td>否</td>
        <td>否</td>
        <td>好</td>
        <td>否</td>
    </tr>
    <tr>
        <td>3</td>
        <td>青年</td>
        <td>是</td>
        <td>否</td>
        <td>好</td>
        <td>是</td>
    </tr>
    <tr>
        <td>4</td>
        <td>青年</td>
        <td>是</td>
        <td>是</td>
        <td>一般</td>
        <td>是</td>
    </tr>
    <tr>
        <td>5</td>
        <td>青年</td>
        <td>否</td>
        <td>否</td>
        <td>一般</td>
        <td>否</td>
    </tr>
    <tr>
        <td>6</td>
        <td>中年</td>
        <td>否</td>
        <td>否</td>
        <td>一般</td>
        <td>否</td>
    </tr>
    <tr>
        <td>7</td>
        <td>中年</td>
        <td>否</td>
        <td>否</td>
        <td>好</td>
        <td>否</td>
    </tr>
    <tr>
        <td>8</td>
        <td>中年</td>
        <td>是</td>
        <td>是</td>
        <td>好</td>
        <td>是</td>
    </tr>
    <tr>
        <td>9</td>
        <td>中年</td>
        <td>否</td>
        <td>是</td>
        <td>非常好</td>
        <td>是</td>
    </tr>
    <tr>
        <td>10</td>
        <td>中年</td>
        <td>否</td>
        <td>是</td>
        <td>非常好</td>
        <td>是</td>
    </tr>
    <tr>
        <td>11</td>
        <td>老年</td>
        <td>否</td>
        <td>是</td>
        <td>非常好</td>
        <td>是</td>
    </tr>
    <tr>
        <td>12</td>
        <td>老年</td>
        <td>否</td>
        <td>是</td>
        <td>好</td>
        <td>是</td>
    </tr>
    <tr>
        <td>13</td>
        <td>老年</td>
        <td>是</td>
        <td>否</td>
        <td>好</td>
        <td>是</td>
    </tr>
    <tr>
        <td>14</td>
        <td>老年</td>
        <td>是</td>
        <td>否</td>
        <td>非常好</td>
        <td>是</td>
    </tr>
    <tr>
        <td>15</td>
        <td>中年</td>
        <td>否</td>
        <td>否</td>
        <td>一般</td>
        <td>否</td>
    </tr>
</table>

希望根据所给的训练数据学习一个贷款申请的决策树，用以对未来的贷款申请进行分类，即当新的客户提出贷款申请时，根据申请人的特征利用决策树决定是否批准贷款申请。

特征选择就是决定用哪个特征来划分特征空间。比如我们通过上述表格得到两个可能的决策树，分别由两个不同特征的根结点构成。

![image](https://github.com/Zhang-Sun/My-first-proj/blob/机器学习/决策树算法实战/决策树图像与数据/20170721163147661.png)

图（a）所示的根结点的特征是年龄，有三个取值，对应于不同的取值由不同的子结点。图（b）所示的根结点的特征是工作，有两个取值。两个决策树都可以从此延续下去。问题是哪个特征更好一点呢？这就要求确定选择特征的准则。直观上，如果一个特征具有更好的分类能力，或者说，按照这个特征将训练数据集分隔成子集，使得各个子集在当前条件下有最好的分类，那么就更应该选择这个特征。信息增益就能够很好的表示这一直观的准则。

什么是信息增益呢？在划分数据集之前之后的信息发生的变化成为信息增益，知道如何计算信息增益，我们就可以计算每个特征值划分数据集获得的信息增益，获得信息增益的最高的特征就是最好的选择。

#### 2.2.1.1 香农熵

在评测哪些数据划分方式是最好的数据划分之前，我们鼻血学习如何计算信息增益。集合信息的度量方式称为香农熵或者简称为熵（entropy）。

熵定义为信息的期望值。在信息论与概率统计中，熵是表示随机变量不确定性的度量。如果待分类的事物可能划分在多个分类之中，则符号xi的信息定义为：$$ l(x_i)=-\log_2p(x_i) $$

其中p(xi)是选择该分类的概率。信息的定义如此，是既定的事实，因此需要我们牢记并会使用。上式中以2为底，也可以e为底。

通过上式，我们可以得到所有类别的信息。为了计算熵，我们需要计算所有类别的所有可能值包含的信息期望值，通过下式得到 $$ H = -\sum\limits_{i=1}^np(x_i)\log_2p(x_i) $$

 其中n式分类的数目。熵越大，随机变量的不确定性越大。

当熵中的概率由数据估计（特别是极大似然估计）得到时，对应的熵称为经验熵。什么叫由数据估计？比如有10个数据，一共有两个类别，A类和B类。其中有7个数据属于A类，则该A类的概率即为十分之七，其中有3个数据属于B类，则该B类的概率即为十分之三。浅显的说，就是这个概率是我们根据数据数出来的。我们定义贷款申请样本数据表中的数据为训练数据集D，则训练数据集D的经验熵为H(D)，| D |表示其样本容量，即样本个数。设有K个类Ck，k=1，2，3，4.......K，则 | Ck |为属于类Ck的样本的个数，这经验熵可以写为：$$ H(D) = -\sum\limits_{k=1}^K\frac{|C_k|}{|D|}\log_2\frac{|C_k|}{|D|} $$ 

根据此公式计算信息熵H(D)，分析贷款申请样本数据表中的数据。最终分类结果只有两类，即放贷和不放贷。根据表中的数据统计可知，在15个数据中，9个数据的结果为放贷，6个数据的结果为不放贷。所以数据集D中的经验熵为：$$ H(D) = -\frac{9}{15}\log_2\frac{9}{15}-\frac{6}{15}\log_2\frac{6}{15}=0.971 $$

经过计算可知，数据集D的经验熵为0.971

#### 2.2.1.2 编写代码计算经验熵

在编写代码之前，我们先对数据集进行属性标注

* 年龄：0代表青年，1代表中年，2代表老年
* 有工作：0代表否，1代表是
* 有自己的房子：0代表否，1代表是
* 信贷情况：0代表一般，1代表好，2代表非常好
* 类别：no代表否，yes代表是

确定这些以后我们就可以创建数据集，并计算经验熵了，代码编写如下：


```python
# -*- coding：UTF-8 -*-
from math import log
"""
函数说明：创建测试数据集

Parameters：
        None

Returns：
        dataSet -数据集
        labels - 分类属性
        
Created：
        2020-4-15
"""
def createDataSet():
    dataSet = [[0,0,0,0,'no'],
              [0,0,0,1,'no'],
              [0,1,0,1,'yes'],
              [0,1,1,0,'yes'],
              [0,0,0,0,'no'],
              [1,0,0,0,'no'],
              [1,0,0,1,'no'],
              [1,1,1,1,'yes'],
              [1,0,1,2,'yes'],
              [1,0,1,2,'yes'],
              [2,0,1,2,'yes'],
              [2,0,1,1,'yes'],
              [2,1,0,1,'yes'],
              [2,1,0,2,'yes'],
              [2,0,0,0,'no']]
    labels = ['年龄','有工作','有自己的房子','信贷情况']      # 分类属性
    return dataSet, labels                                          # 返回数据集和分类属性

"""
函数说明：计算给定数据集的经验熵

Parameters：
        dataSet - 数据集

Returns：
        shannonEnt -经验熵
        
Modify：
        2020-4-15
"""
def calShannonEnt(dataSet):
    numEntires = len(dataSet)         #返回数据集的行数
    labelCounts = {}                       #保存每个标签（Label）出现的次数
    for featVec in dataSet:             # 对每组特征向量进行统计
        currentLabel = featVec[-1]   #提取标签信息
        if currentLabel not in labelCounts.keys():  # 如果标签没有在统计次数的字典中，加进去
            labelCounts[currentLabel] = 0;
        labelCounts[currentLabel] += 1                #Label计数
    shannonEnt = 0.0                #经验熵
    for key in labelCounts:       #计算香农熵
        prob = float(labelCounts[key]) / numEntires      #选择该标签的概率
        shannonEnt -= prob * log(prob, 2)               #利用公式计算
    return shannonEnt

if __name__=='__main__':
    dataSet, features = createDataSet()
    print(dataSet)
    print(calShannonEnt(dataSet))
        
```

    [[0, 0, 0, 0, 'no'], [0, 0, 0, 1, 'no'], [0, 1, 0, 1, 'yes'], [0, 1, 1, 0, 'yes'], [0, 0, 0, 0, 'no'], [1, 0, 0, 0, 'no'], [1, 0, 0, 1, 'no'], [1, 1, 1, 1, 'yes'], [1, 0, 1, 2, 'yes'], [1, 0, 1, 2, 'yes'], [2, 0, 1, 2, 'yes'], [2, 0, 1, 1, 'yes'], [2, 1, 0, 1, 'yes'], [2, 1, 0, 2, 'yes'], [2, 0, 0, 0, 'no']]
    0.9709505944546686


#### 2.2.1.3 信息增益

我们已经说过，如何选择特征需要看信息增益。也就是说，信息增益是相对于特征而言的，信息增益越大，特征对最终的分类结果的影响也就越大，我们就应该选择对最终分类结果影响最大的那个特征作为我们分类的特征。

在讲解信息增益的定义之前，我们需要明确一个概念，条件熵。

条件熵H(Y|X)表示在已知随机变量X的条件下随机变量Y的不确定性，随机变量X给定的条件下随机变量Y的条件熵（conditional entropy）H(Y|X),定义X给定条件下Y的条件概率分布的熵对X的数学期望：$$ H(Y|X) = \sum\limits_{i=1}^np_iH(Y|X=x_i) $$

这里$$ p_i =P(X = x_i),i=1,2,...,n$$

同理，当条件熵中的概率由数据估计（特别是极大似然估计）得到时，所得到的经验熵称为条件经验熵。

前面也提到了，信息增益是相对于特征而言的。所以特征A对训练数据集D的信息增益(D，A)，定义为集合D的经验熵H(D)与特征A给定条件下D的经验条件熵H(D|A)之差，即$$g(D,A) = H(D) - H(D|A) $$

一般的，熵H(D)与条件熵H(D|A)之差称为互信息(mutual information)。决策树学习中的信息增益等价于训练数据集中类与特征的互信息。

设特征A有n个不同的取值{a1,a2,a3,...,an}，根据特征A的取值将D划分为n个子集D1，D2，D3，...，Dn，|Di|为Di的样本个数。记子集Di中属于Ck的样本的集合为Dik，即Dik=Di∩Ck，|Dik|为Dik的样本个数，于是经验条件熵的公式可以写为：$$H(D|A) = \sum\limits_{i=1}^n\frac{|D_i|}{|D|}H(D_i)=-\sum\limits_{i=1}^n\frac{|D_i|}{|D|}\sum\limits_{k=1}^K\frac{|D_{ik}|}{|D_i|}\log_2\frac{|D_{ik}|}{|D_i|}$$

下面举几个例子说明这些概念。

以贷款申请样本数据表为例进行说明。看下年龄数据，也就是特征A1，一共有三个类别，分别是青年、中年和老年。我们只看年龄是青年的数据，年龄是青年的数据一共有5个，所以年龄是青年的数据在训练数据集中出现的概率是三分之一。同理，年龄是中年和老年的数据在训练数据集中出现的概率也是三分之一。现在我们只看年龄是青年的数据的最终得到贷款的概率为五分之二，因为在这五个数据中只有两个数据显示拿到了最终贷款，同理，年龄是中年和老年的数据最终得到贷款的概率分别是五分之三个五分之四。所以计算年龄的信息增益，过程如下：$$ g(D,A_1)=H(D)-[\frac{5}{15}H(D_1)+\frac{5}{15}H(D_2)+\frac{5}{15}H(D_3)]$$

$$ =0.971-[\frac{5}{15}(-\frac{2}{5}\log_2\frac{2}{5}-\frac{3}{5}\log_2\frac{3}{5})+\frac{5}{15}(-\frac{3}{5}\log_3\frac{3}{5}-\frac{2}{5}\log_2\frac{2}{5})+\frac{5}{15}(-\frac{4}{5}\log_2\frac{4}{5}-\frac{1}{5}\log_2\frac{1}{5})]$$

$$ =0.971-0.888=0.083$$ 

同理，计算其余的特征的信息增益g(D,A2)、g(D,A3)和g(D,A4)。分别为：$$g(D,A_2)=H(D)-[\frac{5}{15}H(D_1)+\frac{10}{15}H(D_2)]=0.971-[\frac{5}{15}•0+\frac{10}{15}(-\frac{4}{10}\log_2\frac{4}{10}-\frac{6}{10}\log_2\frac{6}{10})]=0.971-0.647=0.324$$

$$g(D,A_3)=H(D)-[\frac{6}{15}H(D_1)+\frac{9}{15}H(D_2)]=0.971-[\frac{6}{15}•0+\frac{9}{15}(-\frac{3}{9}\log_2\frac{3}{9}-\frac{6}{9}\log_2\frac{6}{9})]=0.971-0.551=0.420$$

$$g(D,A_4)=0.971-0.608=0.363$$

最后比较特征的信息增益，由于特征A3（有自己的房子）的信息增益最大，所以选择A3作为最优特征。

#### 2.2.1.4 编写代码计算信息增益

以下代码实现了利用上述信息增益公式计算信息增益选择最优特征。


```python
# -*- coding:UTF-8 -*-
from math import log

"""
函数说明：计算给定数据集的经验熵（香农熵）

Parameters：
        dataSet - 数据集

Returns：
        shannonEnt - 经验熵（香农熵）
        
Modify：
        2020-04-17
"""
def calShannonEnt(dataSet):
    numEntires = len(dataSet)         #返回数据集的行数
    labelCounts = {}                       #保存每个标签（Label）出现的次数
    for featVec in dataSet:             # 对每组特征向量进行统计
        currentLabel = featVec[-1]   #提取标签信息
        if currentLabel not in labelCounts.keys():  # 如果标签没有在统计次数的字典中，加进去
            labelCounts[currentLabel] = 0;
        labelCounts[currentLabel] += 1                #Label计数
    shannonEnt = 0.0                #经验熵
    for key in labelCounts:       #计算香农熵
        prob = float(labelCounts[key]) / numEntires      #选择该标签的概率
        shannonEnt -= prob * log(prob, 2)               #利用公式计算
    return shannonEnt

"""
函数说明：创建测试数据集

Parameters:
        None

Returns:
        dataSet - 数据集
        labels -分类属性
        
Modify：
        2020-4-17
"""

def createDataSet():
    dataSet = [[0,0,0,0,'no'],
              [0,0,0,1,'no'],
              [0,1,0,1,'yes'],
              [0,1,1,0,'yes'],
              [0,0,0,0,'no'],
              [1,0,0,0,'no'],
              [1,0,0,1,'no'],
              [1,1,1,1,'yes'],
              [1,0,1,2,'yes'],
              [1,0,1,2,'yes'],
              [2,0,1,2,'yes'],
              [2,0,1,1,'yes'],
              [2,1,0,1,'yes'],
              [2,1,0,2,'yes'],
              [2,0,0,0,'no']]
    labels = ['年龄','有工作','有自己的房子','信贷情况']      # 分类属性
    return dataSet, labels                                          # 返回数据集和分类属性

"""
函数说明：按照给定的特征划分数据集

Parameters：
        dataSet - 待划分的数据集
        axis -划分数据集的特征
        value - 需要返回的特征的值

Returns：
        retDataSet -划分后的数据集
        
Modify：
        2020-4-17
"""
def splitDataSet(dataSet, axis, value):
    retDataSet = []                               # 创建返回的数据集列表
    for featVec in dataSet:                    #遍历数据集
        if featVec[axis] == value:
            reducedFeatVec = featVec[:axis]  #去掉axis特征
            reducedFeatVec.extend(featVec[axis+1:])  #将符合条件的添加到返回的数据集
            retDataSet.append(reducedFeatVec)
    return retDataSet

"""
函数说明：选择最优特征

parameters：
        dataSet - 数据集
        
Returns：
        bestFeature - 信息增益最大特征的索引值
        
Modify：
        2020-4-17
    
"""
def chooseBestFeatureToSplit(dataSet):
    numFeatures = len(dataSet[0]) -1                #特征的总数量
    baseEntropy = calShannonEnt(dataSet)      #计算数据集的香农熵
    bestInfoGain = 0.0                                   #信息增益
    bestFeature = -1                                      #最优特征的索引值
    for i in range(numFeatures):
        #获取dataSet的第i个所有特征
        featList = [example[i] for example in dataSet]
        uniqueVals = set(featList)            #创建set集合{},特征不可重复
        newEntropy = 0.0                       #经验条件熵
        for value in uniqueVals:             #计算信息增益
            subDataSet = splitDataSet(dataSet, i, value)         #subdataSet划分后的子集
            prob = len(subDataSet) / float(len(dataSet))        #计算子集概率
            newEntropy += prob* calShannonEnt(subDataSet)       #根据公式计算经验条件熵
        infoGain = baseEntropy -newEntropy           #信息增益
        print("第%d个特征的信息增益为%.3f" % (i, infoGain))         #打印每个特征值的信息增益
        if(infoGain > bestInfoGain):
            bestInfoGain = infoGain          #更新信息增益，找到最大的信息增益
            bestFeature = i              #记录信息增益最大的特征的索引值
    return bestFeature

if __name__=='__main__':
    dataSet, features = createDataSet()
    print("最优特征索引值："+ str(chooseBestFeatureToSplit(dataSet)))

            
```

    第0个特征的信息增益为0.083
    第1个特征的信息增益为0.324
    第2个特征的信息增益为0.420
    第3个特征的信息增益为0.363
    最优特征索引值：2


splitDataSet函数是用来选择各个特征的子集的，比如选择年龄（第0个特征）的青年（用0代替）的子集，我们可以调用splitDataSet(dataSet, 0, 0)这样返回的子集就是年龄为青年的5个数据集。

## 2.3决策树的生成和修剪

我们已经学习了从数据集构造决策树所需要的算法模块，包括经验熵的计算和最优特征值的选择，其原理如下：得到原始数据集，然后基于最好的属性值划分数据集，由于特征值可能多于两个，因此在可能存在大于两个分支的数据集划分。第一次划分后，数据集被向下传递到树的分支的下一个结点。在这个节点上，我们再次划分数据集。因此我们可以采用递归的原则处理数据集。

决策树生成算法递归的生成决策树，直到不能继续下去为止。这样产生的决策速滑往往对训练集数据的分类很准确，但对于未知的测试数据的分类就没有那么准确，即出现过拟合现象。过拟合的原因在于学习时过多的考虑如何提高对训练数据的正确分类，从而构建出过于复杂的决策树。解决这个问题的办法是考虑决策树的复杂度，对已生成的决策树进行简化。

## 2.4 决策树的构造

决策树学习的算法通常是一个递归地选择最优特征，并根据该特征对训练数据进行分割，使得各个子数据集有一个最好的分类过程。这一过程对应着对特征空间的划分，也对应着决策树的构建。

1. 开始：构建根结点，将所有训练数据都放在根结点，选择一个最优特征，按着这一特征将训练数据集分割成子集，使得各个子集有一个在当前条件下最好的分类。
2. 如果这些子集已经能够基本被正确分类，那么构建叶结点，并将这些子集分到对应的叶节点去。
3. 如果子集不能被正确分割，那么就对这些子集选择新的最优特征，继续对其进行分割，构建相应的节点。如果递归进行，直至所有训练数据子集被基本正确分类，或者没有合适的特征为止。
4. 每个子集都被分到叶结点上，即都有了明确的类，这样就生成了一颗决策树。

#### 决策树的特点：

* 优点：计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感，可以处理不相关特征数据。
* 缺点：可能会产生过度匹配的问题
* 使用数据类型：数值型和标称型

本节使用ID3算法来划分数据集，该算法处理如何划分数据集，何时停止划分数据集。

### 2.4.1 决策树的构建

#### 1.ID3算法

ID3算法的核心是在决策树各个结点上对应信息增益准则选择特征，递归地构建决策树。

具体方法是：
1. 从根结点（root node）开始，对结点计算所有可能的特征的信息增益，选择信息增益最大的特征作为结点的特征。
2. 由该特征的不同取值建立子节点，再对子节点递归的调用以上方法，构建决策树；直到所有特征的信息增益均很小或没有特征可以选择为止。
3. 最后得到一个决策树。

ID3相当于用极大似然法进行概率模型的选择，其算法步骤如下：  
输入：训练数据集D，特征集A，阈值$\varepsilon$;  
输出：决策树T；
1. 若D中所有实例均属于同一类$C_k$,则T为单结点树，并将$C_k$作为该结点的类标记，返回T；
2. 若A=ø，则T为单结点树，并将D中实例数最大的类$C_k$作为该结点的类标记，返回T；
3. 否则，计算特征对D的信息增益，选择信息增益最大的特征$A_g$；
4. 若$A_g$的信息增益小于阈值$\varepsilon$，则置T为单结点树，并将D中实例数最大的类$C_K$作为该结点的类标记，返回T；
5. 否则，对$A_g$的每一个可能取值$a_i$，依$A_g=a_i$将D分隔成若干非空子集$D_i$，将$D_i$中实例数最大的类作为标记，构建子节点，由结点及其子节点构成树T，返回T；
6. 对第i个子节点，以$D_i$为训练集，以A-{$A_g$}为特征集，递归的调用步骤1-5，得到子树$T_i$，返回$T_i$；

分析数据：

上面已经求得，特征A3（有自己的房子）的信息增益最大，所以选择A3作为根结点的特征，它将训练集D划分为两个子集D1（A3的取值是“是”）D2（A3的取值为“否”）。由于D1只有同一类的样本点，所以它成为一个叶结点，结点的类标记为“是”。

对D2则需要从特征A1（年龄），A2（有工作）和A4（信贷情况）中选择新的特征，计算各个特征的信息增益：$$g(D2,A1)=H(D2)-H(D2|A1)=0.251$$  
$$g(D2,A2)=H(D2)-H(D2|A2)=0.918$$  
$$g(D2,A4)=H(D2)-H(D2|A4)=0.474$$


根据计算，选择信息增益最大的A2作为结点的特征，由于其有两个取值可能，所以引出两个子节点：  
1. 对应“是”(有工作)，包含三个样本，属于同一类，所以是一个叶子结点，类标记为“是”
2. 对应“否”(无工作)，包含六个样本，属于同一类，所以是一个叶子结点，类标记为“否”  
这样就生成一个决策树，该决策树只用了两个特征(有两个内部节点)，生辰的决策树如下图所示：

![image](https://github.com/Zhang-Sun/My-first-proj/blob/机器学习/决策树算法实战/决策树图像与数据/IMG_1511.PNG)

#### C4.5的生成算法

与ID3算法相似，但是做了改进，将信息增益比作为选择特征的标准。

算法步骤：

输入：训练数据集D，特征集A，阈值$\varepsilon$；  
输出：决策树T。
1. 若D中所有实例均属于同一类$C_k$,则T为单结点树，并将$C_k$作为该结点的类标记，返回T；
2. 若A=ø，则T为单结点树，并将D中实例数最大的类$C_k$作为该结点的类标记，返回T；
3. 否则，计算A中各个特征对D的信息增益比，选择信息增益比最大的特征$A_g$；
4. 若$A_g$的信息增益比小于阈值$\varepsilon$，则置T为单结点树，并将D中实例数最大的类$C_K$作为该结点的类标记，返回T；
5. 否则，对$A_g$的每一个可能取值$a_i$，依$A_g=a_i$将D分隔成若干非空子集$D_i$，将$D_i$中实例数最大的类作为标记，构建子节点，由结点及其子节点构成树T，返回T；
6. 对第i个子节点，以$D_i$为训练集，以A-{$A_g$}为特征集，递归的调用步骤1-5，得到子树$T_i$，返回$T_i$；

递归构造决策树：  

从数据集构造决策树算法所需的子功能模块工作原理如下：得到原始数据集，然后基于最好的属性值划分数据集，由于特征值可能多于两个，因此可能存在大于两个分支的数据集划分，第一次划分之后，数据集将被向下传递到树分支的下一个结点，在此结点上再次划分数据，因此可以使用递归的原则处理数据集。

递归结束的条件是：

程序遍历完所有划分数据集的属性，或者每个分支下的所有实例都具有相同的分类，如果所有实例具有相同的分类，则得到一个叶子结点或者终止块，任何到达叶子结点的数据必须属于叶子结点的分类。

编写ID3的算法代码：


```python
# -*- coding:UTF-8  _*_
from math import log
import operator

"""
函数说明：计算给定数据集的经验熵（香农熵）

Parameters：
        dataSet - 数据集

Returns：
        shannonEnt - 经验熵
        
Modify：
        2020-04-20
"""
def calShannonEnt(dataSet):
    numEntires = len(dataSet)         #返回数据集的行数
    labelCounts = {}                       #保存每个标签（Label）出现的次数
    for featVec in dataSet:             # 对每组特征向量进行统计
        currentLabel = featVec[-1]   #提取标签信息
        if currentLabel not in labelCounts.keys():  # 如果标签没有在统计次数的字典中，加进去
            labelCounts[currentLabel] = 0;
        labelCounts[currentLabel] += 1                #Label计数
    shannonEnt = 0.0                #经验熵
    for key in labelCounts:       #计算香农熵
        prob = float(labelCounts[key]) / numEntires      #选择该标签的概率
        shannonEnt -= prob * log(prob, 2)               #利用公式计算
    return shannonEnt

"""
函数说明：创建测试数据集

Parameters：
        None
        
Returns：
        dataSet - 数据集
        labels - 分类属性
        
Modify：
        2020-04-20
"""


def createDataSet():
    dataSet = [[0,0,0,0,'no'],
              [0,0,0,1,'no'],
              [0,1,0,1,'yes'],
              [0,1,1,0,'yes'],
              [0,0,0,0,'no'],
              [1,0,0,0,'no'],
              [1,0,0,1,'no'],
              [1,1,1,1,'yes'],
              [1,0,1,2,'yes'],
              [1,0,1,2,'yes'],
              [2,0,1,2,'yes'],
              [2,0,1,1,'yes'],
              [2,1,0,1,'yes'],
              [2,1,0,2,'yes'],
              [2,0,0,0,'no']]
    labels = ['年龄','有工作','有自己的房子','信贷情况']      # 分类属性
    return dataSet, labels                                          # 返回数据集和分类属性

"""
函数说明：按照给定的特征划分数据集

Parameters：
        dataSet - 待划分的数据集
        axis -划分数据集的特征
        value - 需要返回的特征的值

Returns：
        retDataSet -划分后的数据集
        
Modify：
        2020-4-20
"""
def splitDataSet(dataSet, axis, value):
    retDataSet = []                               # 创建返回的数据集列表
    for featVec in dataSet:                    #遍历数据集
        if featVec[axis] == value:
            reducedFeatVec = featVec[:axis]  #去掉axis特征
            reducedFeatVec.extend(featVec[axis+1:])  #将符合条件的添加到返回的数据集
            retDataSet.append(reducedFeatVec)
    return retDataSet

"""
函数说明：选择最优特征

parameters：
        dataSet - 数据集
        
Returns：
        bestFeature - 信息增益最大特征的索引值
        
Modify：
        2020-4-17
    
"""
def chooseBestFeatureToSplit(dataSet):
    numFeatures = len(dataSet[0]) -1                #特征的总数量
    baseEntropy = calShannonEnt(dataSet)      #计算数据集的香农熵
    bestInfoGain = 0.0                                   #信息增益
    bestFeature = -1                                      #最优特征的索引值
    for i in range(numFeatures):
        #获取dataSet的第i个所有特征
        featList = [example[i] for example in dataSet]
        uniqueVals = set(featList)            #创建set集合{},特征不可重复
        newEntropy = 0.0                       #经验条件熵
        for value in uniqueVals:             #计算信息增益
            subDataSet = splitDataSet(dataSet, i, value)         #subdataSet划分后的子集
            prob = len(subDataSet) / float(len(dataSet))        #计算子集概率
            newEntropy += prob* calShannonEnt(subDataSet)       #根据公式计算经验条件熵
        infoGain = baseEntropy -newEntropy           #信息增益
        print("第%d个特征的信息增益为%.3f" % (i, infoGain))         #打印每个特征值的信息增益
        if(infoGain > bestInfoGain):
            bestInfoGain = infoGain          #更新信息增益，找到最大的信息增益
            bestFeature = i              #记录信息增益最大的特征的索引值
    return bestFeature



"""
函数说明：统计classList中出现次数最多的元素（类标签）

Parameters：
        classList - 类标签列表

Returns：
        sortedClassCount[0][0] - 出现次数最多的元素（类标签）
        
Modify：
        2020-04-20
"""

def majorityCnt(classList):
    classCount = {}
    #统计classList中每个元素出现的次数
    for vote in classList:
        if vote not in classCount.keys():
            classCount[vote] = 0
            classCount[vote] += 1
        #根据字典的值的降序排列
        sortedClassCount = sorted(classCount.items(), key=operator.itemgetter(1), reverse=True)
        return sortedClassCount[0][0]

    
    
"""
函数说明：创建决策树

Parameters：
        dataSet - 训练数据集
        labels - 分类属性标签
        featLabels - 存储选择的最优特征标签
        
Returns：
        myTree - 决策树
        
Modify：
        2020-04-20
"""
def createTree(dataSet, labels, featLabels):
    #分类标签（是否放贷：yes or no）
    classList = [example[-1] for example in dataSet]
    #如果类别完全相同，则停止继续划分
    if classList.count(classList[0] )== len(classList):
        return classList[0]
    #遍历完所有特征时返回出现次数最多的分类标签
    if len(dataSet[0]) == 1:
        return majorityCnt(classList)
    #选择最优特征
    bestFeat = chooseBestFeatureToSplit(dataSet)
    #最优特征的标签
    bestFeatLabel = labels[bestFeat]
    featLabels.append(bestFeatLabel)
    #根据最优特征的标签生成树
    myTree = { bestFeatLabel:{}}
    #删除已经使用的特征标签
    del(labels[bestFeat])
    #得到训练集中所有最优特征的属性值
    featValues=[example[bestFeat] for example in dataSet]
    #去掉重复的属性值
    uniqueVls = set(featValues)
    #遍历特征，创建决策树
    for value in uniqueVls:
        myTree[bestFeatLabel][value] = createTree(splitDataSet(dataSet, bestFeat, value), labels, featLabels)
    return myTree

if __name__=='__main__':
    dataSet, labels = createDataSet()
    featLabels = []
    myTree = createTree(dataSet, labels, featLabels)
    print(myTree)
```

    第0个特征的信息增益为0.083
    第1个特征的信息增益为0.324
    第2个特征的信息增益为0.420
    第3个特征的信息增益为0.363
    第0个特征的信息增益为0.252
    第1个特征的信息增益为0.918
    第2个特征的信息增益为0.474
    {'有自己的房子': {0: {'有工作': {0: 'no', 1: 'yes'}}, 1: 'yes'}}


### 2.4.2决策树的剪枝 

决策树生成算法通过递归产生决策树，直到不能继续下去为止，这样产生的树往往对训练数据的分类很准确，但对未知测试数据的分类却没有那么准确，即会出现过拟合的现象。过拟合产生的原因在于在学习时过多的考虑如何提高对训练数据的正确分类，从而构建出过于复杂的决策树，解决方法是考虑决策树的复杂度，对已生成的决策树进行简化

剪枝（pruning）：从已生成的树上裁掉一些子树或叶结点，并将其根结点或父节点作为新的叶子结点，从而简化分类树模型。

实现方式：极小化决策树整体的损失函数或代价函数来实现

决策树学习的损失函数定义为：$$Ca(T)=\sum\limits_{t=1}^{|T|}N_tH_t(T)+a|T|$$

其中
<table>
    <tr>
        <td>参数</td>
        <td>意义</td>
    </tr>
    <tr>
        <td>T</td>
        <td>表示这棵树的叶子结点</td>
    </tr>
    <tr>
        <td>Ht(T)</td>
        <td>表示第t个叶子的熵</td>
    </tr>
    <tr>
        <td>Nt</td>
        <td>表示该叶子所含的训练样例的个数</td>
    </tr>
    <tr>
        <td>a</td>
        <td>惩罚系数</td>
    </tr>
</table>

因为：
$$C(T)=\sum\limits_{t=1}^{|T|}N_tH_t(T)=-\sum\limits_{t=1}^{|T|}\sum\limits_{k=1}^KN_{tk}\log\frac{N_{tk}}{N_t}$$

所以：
$$Ca(T)=C(T)+a|T|$$

其中：
<table>
    <tr>
        <td>参数</td>
        <td>意义</td>
    </tr>
    <tr>
        <td>C(T)</td>
        <td>表示模型对训练数据的预测误差，即模型与训练数据的拟合程度</td>
    </tr>
    <tr>
        <td>a</td>
        <td>参数a>=0控制两者之间的影响，较大的a促使选择较简单的模型，较小的a促使选择较复杂的模型，a=0意味着只考虑模型与训练时数据的拟合程度，不考虑模型的复杂度</td>
    </tr>
</table>

剪枝就是当a确定时，选择损失函数最小的模型，即损失函数最小的子树
* 当a确定时，子树越大，往往与训练数据拟合的越好，但是模型的复杂度越高。
* 子树越大，模型的复杂读也就越低，但是往往与训练数据的拟合不好。
* 损失函数正好表示类对二者的平衡

损失函数认为对于每个分类终点(叶子结点)的不确定性程度就是分类的损失因子，而叶子结点的个数是模型的复杂程度，作为惩罚项，损失函数的第一项是样本的训练误差，第二项是模型的复杂度。如果一棵子树的损失函数值越大，说明这棵子树越差，因此我们希望让每一棵子树的损失函数尽可能的小，损失函数最小化就是用正则化的极大似然估计进行模型的选择的过程。

决策树的剪枝过程（泛化过程）就是从叶子结点开始递归，记其父节点将所有子节点回缩后的子树为Tb（分类值取类别比例最大的特征值），未回缩的子树为Ta，如果Ca(Ta)>=Ca(Tb),说明回缩后使损失函数减小了，那么应该使这棵子树回缩，递归直到无法回缩为止，这样使用贪婪的思想进行剪枝可以降低损失函数值，也使决策树得到泛化。

可以看出，决策树的生成只考虑通过提高信息增益对训练数据进行更好的拟合，而决策树剪枝通过优化损失函数还考虑了减小模型的复杂度。

#### ID3、C4.5、CART算法的区别

这三个是非常著名的决策树算法。简单来说，ID3使用信息增益作为选择特征的准则；C4.5使用信息增益比作为选择特征的准则；CART使用Gini指数作为选择特征的准则。

1. ID3

熵表示的是数据中包含的信息量的大小。熵越小，数据的纯度越高，也就是说数据越趋于一致，这是我们希望的划分数据之后的样子。  
  
  
信息增益 = 划分前熵 - 划分后熵。信息增益越大，则意味着使用属性a来进行划分所获的的“纯度提升”越大。也就是说，用属性a来划分训练集，得到的结果中纯度比较高。  
  
  
ID3仅仅适用于二分类问题。ID3仅仅能够处理离散型数据。

2. C4.5

C4.5克服了ID3仅仅能够处理离散属性的问题，以及信息增益偏向选择取值较多特征的问题，使用信息增益比来选择特征。信息增益比 = 信息增益 / 划分前熵  选择信息增益比最大的作为最优特征。

C4.5处理连续性特征是先将特征取值排序，以连续两个值中间值作为划分标准。尝试每一种划分，并计算修正后的信息增益，选择信息增益最大的分裂点作为该属性的分裂点。

3. CART

CART与ID3，C4.5不同之处在于CART生成的树必须是二叉树。也就是说，无论回归还是分类问题，无论特征是离散的还是连续的，无论属性取值有多个还是两个，内部节点只能根据属性值进行二分。

CART的全称是分类与回归树，从这个名字中就知道，CART既可以用于分类问题，也可以用于回归问题。

回归树中，使用平方误差最小化的准则来选择特征并进行划分。每个叶子结点给出的预测值，是划分到该叶子结点的所有样本目标的均值，这样只是在给定划分的情况下最小化了平方误差。

要确定最优划分，还需要遍历所有属性，以及其所有的取值来分别尝试划分并计算在这种划分下的最小平方误差，选取最小的作为此次划分的依据。由于回归树生成使用平方误差最小化准则，所以又叫做最小二乘回归树。

分类树，使用Gini指数最小化的准则来选择特征并进行划分；

Gini指数表示集合的不确定性，或者是不纯度。基尼指数越大，集合的不确定性越高，不纯度也就越大。这一点和熵类似。另一种理解基尼指数的思路是，基尼指数是为了最小化误分类的频概率。

#### 信息增益比VS信息增益

之所以引入信息增益比，是由于信息增益的一个缺点。那就是：信息增益总是偏向于选择取值较多的属性。信息增益比在此基础上增加了一个罚项，解决这个问题。

#### Gini指数VS熵

二者都可以表示数据的不确定性，不纯度。二者的差别如下所示：
* Gni指数的计算不设计对数运算，更加高效；
* Gini指数更偏向于连续属性，熵更偏向于离散属性；

### 2.4.3使用决策树进行分类

依靠训练数据构造出决策树后，我们将它用于实际数据的分类。在执行数据分类时，需要决策树以及用于构造树的标签向量。然后，程序比较预测数据与决策树上的数值，递归执行该过程直到进入叶子结点；最后将测试数据定义为叶子结点所属的类型。从构造决策树的代码，我们可以看到，有个featLabels的输入参数。它是用来记录各个分类节点的，在用决策树做预测的时候，我们按顺序输入需要的分类节点属性值即可。举个例子，比如我用上述已经训练好的决策树做分类，那么我只需提供这个人是否有房子，是否有工作这两个信息即可，无需提供冗余的信息。


```python
# -*- coding:UTF-8  _*_
from math import log
import operator

"""
函数说明：计算给定数据集的经验熵（香农熵）

Parameters：
        dataSet - 数据集

Returns：
        shannonEnt - 经验熵
        
Modify：
        2020-04-20
"""
def calShannonEnt(dataSet):
    numEntires = len(dataSet)         #返回数据集的行数
    labelCounts = {}                       #保存每个标签（Label）出现的次数
    for featVec in dataSet:             # 对每组特征向量进行统计
        currentLabel = featVec[-1]   #提取标签信息
        if currentLabel not in labelCounts.keys():  # 如果标签没有在统计次数的字典中，加进去
            labelCounts[currentLabel] = 0;
        labelCounts[currentLabel] += 1                #Label计数
    shannonEnt = 0.0                #经验熵
    for key in labelCounts:       #计算香农熵
        prob = float(labelCounts[key]) / numEntires      #选择该标签的概率
        shannonEnt -= prob * log(prob, 2)               #利用公式计算
    return shannonEnt

"""
函数说明：创建测试数据集

Parameters：
        None
        
Returns：
        dataSet - 数据集
        labels - 分类属性
        
Modify：
        2020-04-20
"""


def createDataSet():
    dataSet = [[0,0,0,0,'no'],
              [0,0,0,1,'no'],
              [0,1,0,1,'yes'],
              [0,1,1,0,'yes'],
              [0,0,0,0,'no'],
              [1,0,0,0,'no'],
              [1,0,0,1,'no'],
              [1,1,1,1,'yes'],
              [1,0,1,2,'yes'],
              [1,0,1,2,'yes'],
              [2,0,1,2,'yes'],
              [2,0,1,1,'yes'],
              [2,1,0,1,'yes'],
              [2,1,0,2,'yes'],
              [2,0,0,0,'no']]
    labels = ['年龄','有工作','有自己的房子','信贷情况']      # 分类属性
    return dataSet, labels                                          # 返回数据集和分类属性

"""
函数说明：按照给定的特征划分数据集

Parameters：
        dataSet - 待划分的数据集
        axis -划分数据集的特征
        value - 需要返回的特征的值

Returns：
        retDataSet -划分后的数据集
        
Modify：
        2020-4-20
"""
def splitDataSet(dataSet, axis, value):
    retDataSet = []                               # 创建返回的数据集列表
    for featVec in dataSet:                    #遍历数据集
        if featVec[axis] == value:
            reducedFeatVec = featVec[:axis]  #去掉axis特征
            reducedFeatVec.extend(featVec[axis+1:])  #将符合条件的添加到返回的数据集
            retDataSet.append(reducedFeatVec)
    return retDataSet

"""
函数说明：选择最优特征

parameters：
        dataSet - 数据集
        
Returns：
        bestFeature - 信息增益最大特征的索引值
        
Modify：
        2020-4-17
    
"""
def chooseBestFeatureToSplit(dataSet):
    numFeatures = len(dataSet[0]) -1                #特征的总数量
    baseEntropy = calShannonEnt(dataSet)      #计算数据集的香农熵
    bestInfoGain = 0.0                                   #信息增益
    bestFeature = -1                                      #最优特征的索引值
    for i in range(numFeatures):
        #获取dataSet的第i个所有特征
        featList = [example[i] for example in dataSet]
        uniqueVals = set(featList)            #创建set集合{},特征不可重复
        newEntropy = 0.0                       #经验条件熵
        for value in uniqueVals:             #计算信息增益
            subDataSet = splitDataSet(dataSet, i, value)         #subdataSet划分后的子集
            prob = len(subDataSet) / float(len(dataSet))        #计算子集概率
            newEntropy += prob* calShannonEnt(subDataSet)       #根据公式计算经验条件熵
        infoGain = baseEntropy -newEntropy           #信息增益
        print("第%d个特征的信息增益为%.3f" % (i, infoGain))         #打印每个特征值的信息增益
        if(infoGain > bestInfoGain):
            bestInfoGain = infoGain          #更新信息增益，找到最大的信息增益
            bestFeature = i              #记录信息增益最大的特征的索引值
    return bestFeature



"""
函数说明：统计classList中出现次数最多的元素（类标签）

Parameters：
        classList - 类标签列表

Returns：
        sortedClassCount[0][0] - 出现次数最多的元素（类标签）
        
Modify：
        2020-04-20
"""

def majorityCnt(classList):
    classCount = {}
    #统计classList中每个元素出现的次数
    for vote in classList:
        if vote not in classCount.keys():
            classCount[vote] = 0
            classCount[vote] += 1
        #根据字典的值的降序排列
        sortedClassCount = sorted(classCount.items(), key=operator.itemgetter(1), reverse=True)
        return sortedClassCount[0][0]

    
    
"""
函数说明：创建决策树

Parameters：
        dataSet - 训练数据集
        labels - 分类属性标签
        featLabels - 存储选择的最优特征标签
        
Returns：
        myTree - 决策树
        
Modify：
        2020-04-20
"""
def createTree(dataSet, labels, featLabels):
    #分类标签（是否放贷：yes or no）
    classList = [example[-1] for example in dataSet]
    #如果类别完全相同，则停止继续划分
    if classList.count(classList[0] )== len(classList):
        return classList[0]
    #遍历完所有特征时返回出现次数最多的分类标签
    if len(dataSet[0]) == 1:
        return majorityCnt(classList)
    #选择最优特征
    bestFeat = chooseBestFeatureToSplit(dataSet)
    #最优特征的标签
    bestFeatLabel = labels[bestFeat]
    featLabels.append(bestFeatLabel)
    #根据最优特征的标签生成树
    myTree = { bestFeatLabel:{}}
    #删除已经使用的特征标签
    del(labels[bestFeat])
    #得到训练集中所有最优特征的属性值
    featValues=[example[bestFeat] for example in dataSet]
    #去掉重复的属性值
    uniqueVls = set(featValues)
    #遍历特征，创建决策树
    for value in uniqueVls:
        myTree[bestFeatLabel][value] = createTree(splitDataSet(dataSet, bestFeat, value), labels, featLabels)
    return myTree



"""
函数说明：使用决策树进行分类

Parameters：
        inputTree - 已经生成的决策树
        featLabels - 存储选择的最优特征标签
        testVec - 测试数据列表，顺序对应最优特征标签
        
Returns：
        classLabel - 分类结果
        
Modify：
        2020 -04-21
"""
def classify(inputTree, featLabels, testVec):
    #获取决策树的节点
    firstStr = next(iter(inputTree))
    #下一个字典
    secondDict = inputTree[firstStr]
    featIndex = featLabels.index(firstStr)
    
    for key in secondDict.keys():
        if testVec[featIndex] ==key:
            if type(secondDict[key]).__name__=='dict':
                classLabel = classify(secondDict[key], featLabels, testVec)
            else:
                classLabel = secondDict[key]
    return classLabel

if __name__=='__main__':
    dataSet, labels = createDataSet()
    featLabels = []
    myTree = createTree(dataSet, labels, featLabels)
    #测试数据
    testVec = [0,1]
    result = classify(myTree, featLabels, testVec)
    
    if result == 'yes':
        print('放贷')
    if result == 'no':
        print("不放贷")
        

```

    第0个特征的信息增益为0.083
    第1个特征的信息增益为0.324
    第2个特征的信息增益为0.420
    第3个特征的信息增益为0.363
    第0个特征的信息增益为0.252
    第1个特征的信息增益为0.918
    第2个特征的信息增益为0.474
    放贷


### 2.4.4决策树的存储

构造决策树是很耗时的任务，即使处理很小的数据集，如前面的样本数据，也需要花费几秒的时间，如果数据集很大，将会耗费更多的计算时间。然而用创建好的决策树解决分类问题，则可以很快的完成。因此为了节省计算时间，最好能够每次执行分类时调用已经构造好的决策树。为了解决这个问题，需要使用Python模块pickle序列化对象。序列化对象可以在磁盘上保存，并在需要的时候读取出来。

假设我们已经得到决策树{"有自己的房子':{0:{'有工作':{0:'no',1:'yes'}},1:'yes'}},使用pickle.dump存储决策树。


```python
# -*- coding：UTF-8 -*-
import pickle
"""
函数说明：存储决策树

Parameters：
        inputTree - 已经生成的决策树
        filename - 决策树存储文件名
        
Returns：
        None
        
Modify：
        2020-04-21
"""
def storeTree(inputTree, filename):
    with open(filename,'wb') as fw:
        pickle.dump(inputTree,fw)

        
if __name__=='__main__':
    myTree = {'有自己的房子':{0:{'有工作':{0:'no',1:'yes'}},1:'yes'}}
    storeTree(myTree, 'classifierStorage.txt')
```

运行后，在该Python文件的相同目录下，会生成一个名为classifierStorage.txt的txt文件，这个文件存储着我们的决策树。

当要使用存储的决策树时，使用pickle.load进行载入即可，编写代码如下：


```python
# -*- coding:UTF-8 -*-
import pickle

"""
函数说明：读取决策树

Parameters：
            filename - 决策树的存储文件名
            
Returns：
            pickle.load(fr) - 决策树字典
            
Modify：
            2020-04-21
"""
def grabTree(filename):
    fr = open(filename, 'rb')
    return pickle.load(fr)

if __name__=='__main__':
    myTree = grabTree('classifierStorage.txt')
    print(myTree)
```

    {'有自己的房子': {0: {'有工作': {0: 'no', 1: 'yes'}}, 1: 'yes'}}


### 2.4.5 sklearn - 使用决策树预测隐形眼镜的类型

### 步骤

收集数据：使用书中提供的小型数据集

准备数据：对文本中的数据进行预处理，如解析数据行

分析数据：快速检查数据，并使用createPlot()函数绘制最终的树形图

训练决策树：使用CreateTree()函数训练

测试决策树：编写简单的测试函数验证决策树的输出结果&绘图结果

使用决策树：这部分可选择训练好的决策树进行存储，以便随时使用

![image](https://github.com/Zhang-Sun/My-first-proj/blob/机器学习/决策树算法实战/决策树图像与数据/隐形眼镜数据集.png)

#### 2.4.5.1 使用sklearn构建决策树

sklearn.tree ------提供了决策树模型，用于解决分类和回归问题

class sklearn.tree.DecisionTreeClassifier(criterion=’gini’, splitter=’best’, max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, class_weight=None, presort=False)

参数说明如下：

**criterion**：特征选择标准，可选参数，默认是gini，可以设置为entropy。gini是基尼不纯度，是将来自集合的某种结果随机应用于某一数据项的预期误差率，是一种基于统计的思想。entropy是香农熵，也就是上篇文章讲过的内容，是一种基于信息论的思想。Sklearn把gini设为默认参数，应该也是做了相应的斟酌的，精度也许更高些？ID3算法使用的是entropy，CART算法使用的则是gini。

**splitter**：特征划分点选择标准，可选参数，默认是best，可以设置为random。每个结点的选择策略。best参数是根据算法选择最佳的切分特征，例如gini、entropy。random随机的在部分划分点中找局部最优的划分点。默认的”best”适合样本量不大的时候，而如果样本数据量非常大，此时决策树构建推荐”random”。

**max_features**：划分时考虑的最大特征数，可选参数，默认是None。寻找最佳切分时考虑的最大特征数(n_features为总共的特征数)，有如下6种情况：
如果max_features是整型的数，则考虑max_features个特征；
如果max_features是浮点型的数，则考虑int(max_features * n_features)个特征；
如果max_features设为auto，那么max_features = sqrt(n_features)；
如果max_features设为sqrt，那么max_featrues = sqrt(n_features)，跟auto一样；
如果max_features设为log2，那么max_features = log2(n_features)；
如果max_features设为None，那么max_features = n_features，也就是所有特征都用。
一般来说，如果样本特征数不多，比如小于50，我们用默认的”None”就可以了，如果特征数非常多，我们可以灵活使用刚才描述的其他取值来控制划分时考虑的最大特征数，以控制决策树的生成时间。

**max_depth**：决策树最大深，可选参数，默认是None。这个参数是这是树的层数的。层数的概念就是，比如在贷款的例子中，决策树的层数是2层。如果这个参数设置为None，那么决策树在建立子树的时候不会限制子树的深度。一般来说，数据少或者特征少的时候可以不管这个值。或者如果设置了min_samples_slipt参数，那么直到少于min_smaples_split个样本为止。如果模型样本量多，特征也多的情况下，推荐限制这个最大深度，具体的取值取决于数据的分布。常用的可以取值10-100之间。

**min_samples_split**：内部节点再划分所需最小样本数，可选参数，默认是2。这个值限制了子树继续划分的条件。如果min_samples_split为整数，那么在切分内部结点的时候，min_samples_split作为最小的样本数，也就是说，如果样本已经少于min_samples_split个样本，则停止继续切分。如果min_samples_split为浮点数，那么min_samples_split就是一个百分比，ceil(min_samples_split * n_samples)，数是向上取整的。如果样本量不大，不需要管这个值。如果样本量数量级非常大，则推荐增大这个值。  

**min_weight_fraction_leaf**：叶子节点最小的样本权重和，可选参数，默认是0。这个值限制了叶子节点所有样本权重和的最小值，如果小于这个值，则会和兄弟节点一起被剪枝。一般来说，如果我们有较多样本有缺失值，或者分类树样本的分布类别偏差很大，就会引入样本权重，这时我们就要注意这个值了。

**max_leaf_nodes**：最大叶子节点数，可选参数，默认是None。通过限制最大叶子节点数，可以防止过拟合。如果加了限制，算法会建立在最大叶子节点数内最优的决策树。如果特征不多，可以不考虑这个值，但是如果特征分成多的话，可以加以限制，具体的值可以通过交叉验证得到。

**class_weight**：类别权重，可选参数，默认是None，也可以字典、字典列表、balanced。指定样本各类别的的权重，主要是为了防止训练集某些类别的样本过多，导致训练的决策树过于偏向这些类别。类别的权重可以通过{class_label：weight}这样的格式给出，这里可以自己指定各个样本的权重，或者用balanced，如果使用balanced，则算法会自己计算权重，样本量少的类别所对应的样本权重会高。当然，如果你的样本类别分布没有明显的偏倚，则可以不管这个参数，选择默认的None。

**random_state**：可选参数，默认是None。随机数种子。如果是证书，那么random_state会作为随机数生成器的随机数种子。随机数种子，如果没有设置随机数，随机出来的数与当前系统时间有关，每个时刻都是不同的。如果设置了随机数种子，那么相同随机数种子，不同时刻产生的随机数也是相同的。如果是RandomState instance，那么random_state是随机数生成器。如果为None，则随机数生成器使用np.random。

**min_impurity_split**：节点划分最小不纯度,可选参数，默认是1e-7。这是个阈值，这个值限制了决策树的增长，如果某节点的不纯度(基尼系数，信息增益，均方差，绝对差)小于这个阈值，则该节点不再生成子节点。即为叶子节点 。

**presort**：数据是否预排序，可选参数，默认为False，这个值是布尔值，默认是False不排序。一般来说，如果样本量少或者限制了一个深度很小的决策树，设置为true可以让划分点选择更加快，决策树建立的更加快。如果样本量太大的话，反而没有什么好处。问题是样本量少的时候，我速度本来就不慢。所以这个值一般懒得理它就可以了。
除了这些参数要注意以外，其他在调参时的注意点有：

当样本数量少但是样本特征非常多的时候，决策树很容易过拟合，一般来说，样本数比特征数多一些会比较容易建立健壮的模型  
如果样本数量少但是样本特征非常多，在拟合决策树模型前，推荐先做维度规约，比如主成分分析（PCA），特征选择（Losso）或者独立成分分析（ICA）。这样特征的维度会大大减小。再来拟合决策树模型效果会好。  
推荐多用决策树的可视化，同时先限制决策树的深度，这样可以先观察下生成的决策树里数据的初步拟合情况，然后再决定是否要增加深度。  
在训练模型时，注意观察样本的类别情况（主要指分类树），如果类别分布非常不均匀，就要考虑用class_weight来限制模型过于偏向样本多的类别。  
决策树的数组使用的是numpy的float32类型，如果训练数据不是这样的格式，算法会先做copy再运行。  
如果输入的样本矩阵是稀疏的，推荐在拟合前调用csc_matrix稀疏化，在预测前调用csr_matrix稀疏化。


**sklearn.tree.DecisionTreeClassifier()** 提供了一系列方法供我们使用，包括：

**apply(X[,check_input])**              Returns the index of the leaf that each sample is predicted as  
**decision_path(X[,check_input])**                return the decision path in the tree  
**fit(X,y[,sample_weight,check_input,...])**.    Build a decision tree classifier from the training set(X,y)  
**get_params([deep])**                              Get parameters for this estimator.  
**predict(X[,check_input])**   Predict class or regression value for X  
**predict_log_proba(X)**     Predict class log-probabilities of the input samples X  
**predict_proba(X[,check_input])**  Predict class probabilities of the input samples X  
**score(X,y[,sample_weight])**  Returns the mean accuracy on the given test data and labels  
**set_params(**params)**   Set the parameters of this estimator

数据预处理：将String类型的数据集进行编码
1. LabelEncoder：将字符串转换为增量值
2. OneHotEncoder： 使用One-of-K算法将字符串转换为整数

为了对string类型的数据序列化，需要生成pandas数据，这样方便我们的序列化工作。这里我们使用的方法是，原始数据->字典->pandas数据，编写代码如下：


```python
#-*- coding:UTF-8 -*-
import pandas as pd
from sklearn.preprocessing import LabelEncoder

# import pydotplus
# from sklearn.externals.six import StringIO

if __name__ == '__main__':
    # 加载文件
    with open('lenses.txt', 'r') as fr:
        # 处理文件
        lenses = [inst.strip().split('\t') for inst in fr.readlines()]
    # 提取每组数据的类别，保存在列表里
    lenses_target = []
    for each in lenses:
        lenses_target.append(each[-1])
    # 特征标签
    lensesLabels = ['age', 'prescript', 'astigmatic', 'tearRate']
    # 保存lenses数据的临时列表
    lenses_list = []
    # 保存lenses数据的字典，用于生成pandas
    lenses_dict = {}
    # 提取信息，生成字典
    for each_label in lensesLabels:
        for each in lenses:
            lenses_list.append(each[lensesLabels.index(each_label)])
        lenses_dict[each_label] = lenses_list
        lenses_list = []
        # 打印字典信息
    print(lenses_dict)
    #生成pandas.DataFrame
    lenses_pd = pd.DataFrame(lenses_dict)
    # print(lenses_pd)
    # 生成pandas.DataFrame
    lenses_pd = pd.DataFrame(lenses_dict)
    # 打印pandas.DataFrame
    print(lenses_pd)
    # 创建LabelEncoder()对象，用于序列化
    le = LabelEncoder()
    # 为每一列序列化
    for col in lenses_pd.columns:
        lenses_pd[col] = le.fit_transform(lenses_pd[col])
    print(lenses_pd)

```

    {'age': ['young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'pre', 'pre', 'pre', 'pre', 'pre', 'pre', 'pre', 'pre', 'presbyopic', 'presbyopic', 'presbyopic', 'presbyopic', 'presbyopic', 'presbyopic', 'presbyopic', 'presbyopic'], 'prescript': ['myope', 'myope', 'myope', 'myope', 'hyper', 'hyper', 'hyper', 'hyper', 'myope', 'myope', 'myope', 'myope', 'hyper', 'hyper', 'hyper', 'hyper', 'myope', 'myope', 'myope', 'myope', 'hyper', 'hyper', 'hyper', 'hyper'], 'astigmatic': ['no', 'no', 'yes', 'yes', 'no', 'no', 'yes', 'yes', 'no', 'no', 'yes', 'yes', 'no', 'no', 'yes', 'yes', 'no', 'no', 'yes', 'yes', 'no', 'no', 'yes', 'yes'], 'tearRate': ['reduced', 'normal', 'reduced', 'normal', 'reduced', 'normal', 'reduced', 'normal', 'reduced', 'normal', 'reduced', 'normal', 'reduced', 'normal', 'reduced', 'normal', 'reduced', 'normal', 'reduced', 'normal', 'reduced', 'normal', 'reduced', 'normal']}
               age prescript astigmatic tearRate
    0        young     myope         no  reduced
    1        young     myope         no   normal
    2        young     myope        yes  reduced
    3        young     myope        yes   normal
    4        young     hyper         no  reduced
    5        young     hyper         no   normal
    6        young     hyper        yes  reduced
    7        young     hyper        yes   normal
    8          pre     myope         no  reduced
    9          pre     myope         no   normal
    10         pre     myope        yes  reduced
    11         pre     myope        yes   normal
    12         pre     hyper         no  reduced
    13         pre     hyper         no   normal
    14         pre     hyper        yes  reduced
    15         pre     hyper        yes   normal
    16  presbyopic     myope         no  reduced
    17  presbyopic     myope         no   normal
    18  presbyopic     myope        yes  reduced
    19  presbyopic     myope        yes   normal
    20  presbyopic     hyper         no  reduced
    21  presbyopic     hyper         no   normal
    22  presbyopic     hyper        yes  reduced
    23  presbyopic     hyper        yes   normal
        age  prescript  astigmatic  tearRate
    0     2          1           0         1
    1     2          1           0         0
    2     2          1           1         1
    3     2          1           1         0
    4     2          0           0         1
    5     2          0           0         0
    6     2          0           1         1
    7     2          0           1         0
    8     0          1           0         1
    9     0          1           0         0
    10    0          1           1         1
    11    0          1           1         0
    12    0          0           0         1
    13    0          0           0         0
    14    0          0           1         1
    15    0          0           1         0
    16    1          1           0         1
    17    1          1           0         0
    18    1          1           1         1
    19    1          1           1         0
    20    1          0           0         1
    21    1          0           0         0
    22    1          0           1         1
    23    1          0           1         0


### 2.5总结

优点：

* 易于理解和解释，决策树可以可视化
* 几乎不需要数据预处理。其他方法需要数据标准化，创建虚拟变量和删除确实值。决策树不支持确实值
* 使用树的花费是训练数据点数量的对数
* 可以同时处理数值变量和分类变量。其他方法大都适用于分析一种变量的集合
* 可以处理多值输出变量的问题
* 使用白盒模型。如果一个情况被观察到，使用逻辑判断容易表示这种规则。相反，如果使用的是黑盒模型，结果非常难以理解。
* 即使对真实模型来说，假设无效的情况下，也可以较好的使用

缺点：

* 决策树学习可能创建一个过于复杂的树，并不能很好的预测数据。也就是过拟合。修剪机制（现在不支持），设置一个叶子节点需要的最小样本数量，或者数的最大深度，可以避免过拟合。
* 决策树可能是不稳定的，因为即使非常小的变异，可能会产生一颗完全不同的树。这个问题通过decision trees with an ensemble来缓解。
* 学习一颗最优的决策树是一个NP-完全问题under several aspects of optimality and even for simple concepts。因此，传统决策树算法基于启发式算法，例如贪婪算法，即每个节点创建最优决策。这些算法不能产生一个全家最优的决策树。对样本和特征随机抽样可以降低整体效果偏差。
* 概念难以学习，因为决策树没有很好的解释他们，例如，XOR, parity or multiplexer problems.
* 如果某些分类占优势，决策树将会创建一棵有偏差的树。因此，建议在训练之前，先抽样使样本均衡。



```python

```
