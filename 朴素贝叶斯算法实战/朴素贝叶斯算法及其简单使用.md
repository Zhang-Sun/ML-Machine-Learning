# 3.朴素贝叶斯

朴素贝叶斯（Naive Bayes）算法是基于贝叶斯定理与特征条件独立假设的分类方法，对于给定的训练数据集，首先基于特征条件独立假设学习输入/输出的联合概率分布，然后基于此模型，对给定的输入x，利用贝叶斯定理求出后验概率最大的输出y，朴素贝叶斯算法实现简单，学习与预测的概率都很高。

朴素贝叶斯算法是有监督学习算法，解决的事分类问题，如客户是否有流失、是否值得投资、信用等级评定等多分类问题。该算法的优点在于简单易懂、学习效率高、在某领域的分类问题中能够与决策树、神经网络相媲美。但由于该算法已自变量之间的独立（条件特征独立）性和连续变量的正态性为前提，就会导致算法精度在某种程度上受影响。

## 3.1 基于贝叶斯决策理论的分类方法

* 优点：在数据较少的情况下依然有效，可以处理多类别问题
* 缺点：对于输入数据的准备方式较为敏感
* 适用数据类型：标称型数据

### 3.1.1贝叶斯决策理论

朴素贝叶斯是贝叶斯决策理论的一部分，所以首先了解一下贝叶斯理论。

假设现在我们有一个数据集，它由两类数据组成，数据分布如下图所示：
![image](https://github.com/Zhang-Sun/My-first-proj/blob/机器学习/朴素贝叶斯算法实战/朴素贝叶斯图像与数据/20180314104033603.png)
我们现在用p1(x,y)表示数据(x,y)属于类别1(图中原点表示的类别)的概率，用p2(x,y)表示数据点(x,y)属于类别2(图中三角形表示的类别)的概率，那么对于一个新数据点(x,y)，可以用下面的规则判断它的类别：

* 如果p1(x,y)>p2(x,y)，那么类别为1
* 如果p1(x,y)<p2(x,y)，那么类别为2

也就是说，我们会选择高概率对应的类别。这就是贝叶斯决策理论的核心思想，即选择具有最高概率的决策。

适用决策树不会非常成功，和简单的概率计算相比，KNN计算量太大，因此对于上述问题，最佳选择是概率比较方法。

已经了解了贝叶斯决策理论的核心思想，接下来就学习如何计算p1和p2概率。

### 4.1.2 条件概率

在学习计算p1和p2概率之前，我们需要了解什么是条件概率(Conditional probability)，就是指在事件B发生的情况下，事件A发生的概率，用P(A|B)表示。
![](http://latex.codecogs.com/gif.latex?P(A|B)=\\frac{P({A}\cap{B})}{P(B)})

因此：
![](http://latex.codecogs.com/gif.latex?P(A\\cap{B})=P(A|B)P(B))
同理：![](http://latex.codecogs.com/gif.latex?P(A\\cap{B})=P(B|A)P(A))

所以：![](http://latex.codecogs.com/gif.latex?P(A|B)P(B)=P(B|A)P(A))

即：![](http://latex.codecogs.com/gif.latex?P(A|B)=\\frac{P(B|A)P(A)}{P(B)})

上式即为条件概率的计算公式。

### 3.1.3 全概率公式

除了条件概率以外，在计算p1和p2时，还要用到全概率公式，因此，这里继续推导全概率公式。
![](http://latex.codecogs.com/gif.latex?P(B)=P(B\\cap{A})+P(B\cap{A^\prime}))

已知：![](http://latex.codecogs.com/gif.latex?P(B\\cap{A})=+P(B|A)P(A))

故全概率公式为：![](http://latex.codecogs.com/gif.latex?P(B)=P(B|A)P(A)+P(B|A^\\prime)P(A^\prime))

其含义为：如果A和![](http://latex.codecogs.com/gif.latex?A^\\prime)构成一个样本空间的一个划分，那么事件B的概率就等于A和![](http://latex.codecogs.com/gif.latex?A^\\prime)的概率分别乘以B对这两个事件的条件概率之和。

于是条件概率就有了另一个写法：![](http://latex.codecogs.com/gif.latex?P(A|B)=\\frac{P(B|A)P(A)}{P(B|A)P(A)+P(B|A^\prime)P(A^\prime)})

### 3.1.4 贝叶斯推断

对条件概率进行变形，可以得到如下形式：![](http://latex.codecogs.com/gif.latex?P(A|B)=P(A)\frac{P(B|A)}{P(B)})

我们把P(A)称为“先验概率”(Prior probability)，即在B事件发生之前，我们对A事件概率的一个判断。

P(A|B)称为“后验概率”(Posterior probability)，即在B事件发生之后，我们对A事件概率的重新评估。

P(B|A)/P(B)称为“可能性函数”(Likelyhood)，这是一个调整因子，使得预估概率更接近真实概率。

所以，条件概率可以理解为下式子：  
后验概率=先验概率*调整因子

这就是贝叶斯推断的含义：我们先预估一个“先验概率”，然后加入试验结果，看这个实验到底是增强还是削弱了先验概率，由此得到更加接近事实的“后验概率”。

在这里，如果“可能性函数”P(B|A)/P(B)>1，意味着“先验概率”被增强，事件A的发生的可能性变大；如果“可能性函数”=1，意味着B事件无助于判断事件A的可能性；如果“可能性函数”<1，则意味着"先验概率"被削弱，事件A的可能性变小。

### 3.1.5 朴素贝叶斯

**“朴素”的解释** ：假设各个特征之间相互独立（在贝叶斯分类器上做了简化）。  
朴素贝叶斯的基础假设：  
1. 每个特征相互独立；
2. 每个特征的权重都相等，即对结果的影响程度都相同

朴素贝叶斯的具体实现步骤：

1. 假设某个体有n项特征(Feature)，分别为![](http://latex.codecogs.com/gif.latex?x_1,x_2,x_3,......x_n)。现有k个类别(Category)，分别为![](http://latex.codecogs.com/gif.latex?c_1,c_2,c_3,...,c_k)。贝叶斯分类器就是计算出概率最大的那个分类，也就是求下面这个算式的最大值：![](http://latex.codecogs.com/gif.latex?P(c|x_1,x_2,...,x_n)=\\frac{P(x_1,x_2,x_3,...,x_n|c)P(c)}{P(x_1,x_2,...,x_n)})但由于![](http://latex.codecogs.com/gif.latex?P(x_1,x_2,...,x_n))对所有的类别都相同，可以省略，故变为求解![](http://latex.codecogs.com/gif.latex?P(x_1,x_2,...,x_n|c)P(c))的最大值。
2. 朴素贝叶斯分类器假设所有特征相互独立，因此![](http://latex.codecogs.com/gif.latex?P(x_1,x_2,...,x_n|c)P(c)=P(x_1|c)P(x_2|c)...P(x_n|c)P(c))上式等号右边的每一项，都可以从统计资料中得到，由此就可以计算出每个类别对应的概率，从而找出最大的概率的那个类。虽然“所有特征彼此独立”这个假设，在现实中不太可能成立，但是它可以大大简化计算，而且有研究表明对分类结果的准确性影响不大。  
**注意** ：由于训练数据事有限的，极有可能出现P(xi|c)=0的情形，此时可以利用平滑策略解决零概率问题。


举例说明：

某个医院早上来了六个门诊的病人 ，他们的情况如下表所示：

<html>
    <table>
    <tr>
        <td>症状</td>
        <td>职业</td>
        <td>疾病</td>
    </tr>
    <tr>
        <td>打喷嚏</td>
        <td>护士</td>
        <td>感冒</td>
    </tr>
    <tr>
        <td>打喷嚏</td>
        <td>农夫</td>
        <td>疾过敏</td>
    </tr>
    <tr>
        <td>头痛</td>
        <td>建筑工人</td>
        <td>脑震荡</td>
    </tr>
    <tr>
        <td>头痛</td>
        <td>建筑工人</td>
        <td>感冒</td>
    </tr>
    <tr>
        <td>打喷嚏</td>
        <td>教师</td>
        <td>感冒</td>
    </tr>
    <tr>
        <td>头痛</td>
        <td>教师</td>
        <td>脑震荡</td>
    </tr>
    </table>
</html>

现在又来了一个病人，是一个打喷嚏的建筑工人，那么他患上感冒的概率有多大？

#### 根据贝叶斯定理：

![](http://latex.codecogs.com/gif.latex?P(A|B)=\\frac{P(B|A)P(A)}{P(B)})

#### 可得：

![](http://latex.codecogs.com/gif.latex?P(感冒|打喷嚏&建筑工人)=P(打喷嚏&建筑工人|感冒)P(感冒)/P(打喷嚏&建筑工人))

根据朴素贝叶斯条件独立假设可知，打喷嚏和建筑工人两个特征是独立的，所以：

![](http://latex.codecogs.com/gif.latex?P(感冒|打喷嚏&建筑工人)=P(打喷嚏|感冒)*P(建筑工人|感冒)*P(感冒)/P(打喷嚏&建筑工人))

![](http://latex.codecogs.com/gif.latex?P(感冒|打喷嚏&建筑工人)=0.66*0.33*0.5/(0.5*0.33)=0.66)

因此这个打喷嚏的建筑工人有66%的概率是得了感冒。同理，可以计算这个病人患上过敏或脑震荡的概率。比较这几个概率，就可以知道他最可能患什么病。

这就是贝叶斯分类器的基本方法：在统计资料的基础上，依据某些特质，计算个别类别的概率，从而实现分类。

同样，在编程的时候，如果不需要求出所属类别的具体概率，P(打喷嚏)=0.5和P(建筑工人)=0.33的概率是可以不用求的。

## 4.2 使用朴素贝叶斯进行文档分类

朴素贝叶斯是上节介绍的贝叶斯分类器的一个扩展，是用于文档分分类的常用算法。

#### 朴素贝叶斯的一般过程

1. 收集数据：可以使用任何方法。本章使用RSS源。
2. 准备数据：需要数值型或布尔型数据。
3. 分析数据：有大量特征时，绘制特征作用不大，此时使用直方图效果更好。
4. 训练算法：计算不同的独立特征的条件概率。
5. 测试算法：计算错误率。
6. 使用算法：一个常见的朴素贝叶斯应用是文档分类。可以在任意的分类场景中使用朴素贝叶斯分类器，不一定非要是文本。

以在线社区留言为例。为了不影响社区的发展，我们要屏蔽侮辱性的言论，所以要构建一个快速过滤器，如果某条留言使用了负面或者侮辱性的语言，那么就将该留言标志为内容不当。过滤这类内容是一个很常见的需求。对此问题建立两个类型：侮辱类和非侮辱类，分别使用1和0表示。

我们把文本看成单词向量或者词条向量，也就是说将句子转换为向量。考虑出现所有文档中的单词，再决定将哪些单词纳入词汇表或者说所要的词汇集合，然后必须要将每一篇文档转换为词汇表上的向量。简单起见，我们先假设已经将本文切分完毕，存放到列表中，并对词汇向量进行分类标注。编写代码如下：


```python
# -*- coding:UTF-8 -*-

"""
函数说明：创建实验样本

Parameters：
        None
        
Returns：
        postingList - 实验样本切分的词条
        classVec - 类别标签向量
        
Modify：
        2020-04-23
"""
def loadDataSet():
    #切分词条
    postingList = [['my','dog','has','flea','problems','help','please'],
                  ['maybe','not','take','him','to','dog','park','stupid'],
                  ['my','dalmation','is','so','cute','I','love','him'],
                  ['stop','posting','stupid','worthless','garbage'],
                  ['mr','licks','ate','my','steak','how','to','stop','him'],
                  ['quit','buying','worthless','dog','food','stupid']]
    # 类别标签向量，1代表侮辱性词汇，0代表不是
    classVec = [0,1,0,1,0,1]
    return postingList, classVec

if __name__=='__main__':
    postingList,classVec = loadDataSet()
    for each in postingList:
        print(each)
    print(classVec)
```

    ['my', 'dog', 'has', 'flea', 'problems', 'help', 'please']
    ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid']
    ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him']
    ['stop', 'posting', 'stupid', 'worthless', 'garbage']
    ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him']
    ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']
    [0, 1, 0, 1, 0, 1]


创建一个词汇表，并将切好的词条转化为词条向量


```python
# -*- coidng:UTF-8 -*-


"""
函数说明：创建实验样本

Parameters：
        None
        
Returns：
        postingList - 实验样本切分的词条
        classVec - 类别标签向量
        
Modify：
        2020-04-23
"""
def loadDataSet():
    #切分词条
    postingList = [['my','dog','has','flea','problems','help','please'],
                  ['maybe','not','take','him','to','dog','park','stupid'],
                  ['my','dalmation','is','so','cute','I','love','him'],
                  ['stop','posting','stupid','worthless','garbage'],
                  ['mr','licks','ate','my','steak','how','to','stop','him'],
                  ['quit','buying','worthless','dog','food','stupid']]
    # 类别标签向量，1代表侮辱性词汇，0代表不是
    classVec = [0,1,0,1,0,1]
    return postingList, classVec


"""
函数说明：增加vocabList词汇表，将inputSet向量化，向量的每个元素为1或0

Parameters：
        vocabList - createVocabList返回的列表
        inputSet - 切分的词条列表
        
Returns：
        returnVec - 文档向量，词集模型
"""
def setOfWords2Vec(vocabList,inputSet):
    #创建一个其中所含元素都为0的向量
    returnVec =[0]*len(vocabList)
    #遍历每个词条
    for word in inputSet:
        #如果词条存在于词汇表中，则置1
        if word in vocabList:
            returnVec[vocabList.index(word)] = 1
        else:
            print("the word:s% is not in my Vocabulary!" % word)
    #返回向量文档
    return returnVec


"""
函数说明：将切分的实验样本词条整理成不重复的词条列表，也就是词汇表

Parameters：
        dataSet - 整理的样本数据集
        
Returns：
        vocabSet - 返回不重复的词条列表，也即是词汇表
        
Modify：
        2020-04-23
"""
def createVocabList(dataSet):
    #创建一个空的不重复的列表
    vocabSet = set([])
    for document in dataSet:
        #取并集
        vocabSet = vocabSet | set(document)
    return list(vocabSet)

if __name__=='__main__':
    postingList,classVec = loadDataSet()
    print("postingList:\n",postingList)
    myVocabList = createVocabList(postingList)
    print("myVocabList:\n",myVocabList)
    trainMat = []
    for postinDoc in postingList:
        trainMat.append(setOfWords2Vec(myVocabList,postinDoc))
    print('trainMat:\n',trainMat)
```

    postingList:
     [['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'], ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'], ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'], ['stop', 'posting', 'stupid', 'worthless', 'garbage'], ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'], ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]
    myVocabList:
     ['ate', 'not', 'help', 'my', 'stupid', 'licks', 'food', 'him', 'I', 'stop', 'posting', 'so', 'flea', 'dog', 'how', 'love', 'worthless', 'maybe', 'steak', 'garbage', 'quit', 'take', 'dalmation', 'please', 'to', 'problems', 'buying', 'has', 'park', 'mr', 'cute', 'is']
    trainMat:
     [[0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0], [0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0], [0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1], [0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]]


从运行结果可以看出，postingList是原始的词条列表，myVocabList是词汇表。myVocabList是所有单词出现的集合，没有重复的元素。词汇表是用来将词条向量化的，一个单词在词汇表中出现过一次，那么就在相应的位置记作1，如果没有出现在相应的位置记作0。trainMat是所有的词条向量组成的列表。它里面存放的是根据myVocabList向量化的词条向量。

我们已经得到了词条向量，接下来，我们就可以通过词条向量训练朴素贝叶斯分类器。


```python
# -*- coding:UTF-8 -*-
import numpy as np
"""
函数说明：创建实验样本

Parameters：
        None
        
Returns：
        postingList - 实验样本切分的词条
        classVec - 类别标签向量
        
Modify：
        2020-04-23
"""
def loadDataSet():
    #切分词条
    postingList = [['my','dog','has','flea','problems','help','please'],
                  ['maybe','not','take','him','to','dog','park','stupid'],
                  ['my','dalmation','is','so','cute','I','love','him'],
                  ['stop','posting','stupid','worthless','garbage'],
                  ['mr','licks','ate','my','steak','how','to','stop','him'],
                  ['quit','buying','worthless','dog','food','stupid']]
    # 类别标签向量，1代表侮辱性词汇，0代表不是
    classVec = [0,1,0,1,0,1]
    return postingList, classVec


"""
函数说明：增加vocabList词汇表，将inputSet向量化，向量的每个元素为1或0

Parameters：
        vocabList - createVocabList返回的列表
        inputSet - 切分的词条列表
        
Returns：
        returnVec - 文档向量，词集模型
"""
def setOfWords2Vec(vocabList,inputSet):
    #创建一个其中所含元素都为0的向量
    returnVec =[0]*len(vocabList)
    #遍历每个词条
    for word in inputSet:
        #如果词条存在于词汇表中，则置1
        if word in vocabList:
            returnVec[vocabList.index(word)] = 1
        else:
            print("the word:s% is not in my Vocabulary!" % word)
    #返回向量文档
    return returnVec


"""
函数说明：将切分的实验样本词条整理成不重复的词条列表，也就是词汇表

Parameters：
        dataSet - 整理的样本数据集
        
Returns：
        vocabSet - 返回不重复的词条列表，也即是词汇表
        
Modify：
        2020-04-23
"""
def createVocabList(dataSet):
    #创建一个空的不重复的列表
    vocabSet = set([])
    for document in dataSet:
        #取并集
        vocabSet = vocabSet | set(document)
    return list(vocabSet)


"""
函数说明：朴素贝叶斯分类器训练函数

Parameters：
        trainMatrix - 训练文档矩阵，即setOfWords2Vec返回的returnVec构成的矩阵
        trainCategory - 训练类别标签向量，即loadDataSet返回的classVec

Returns：
        p0Vect - 侮辱类的条件概率数组
        p1Vect - 非侮辱类的条件概率数组
        pAbusive - 文档属于侮辱类的概率
        
Modify：
        2020-04-25
"""
def trainNB0(trainMatrix, trainCategory):
    #计算训练的文档数目
    numTrainDocs = len(trainMatrix)
    #计算每篇文章的词条数（即单词表单词个数）
    numWords = len(trainMatrix[0])
    #文档属于侮辱类的概率
    pAbusive = sum(trainCategory)/float(numTrainDocs)
    #创建numpy.zeros数组
    p0Num = np.zeros(numWords)
    p1Num = np.zeros(numWords)
    #分母初始化为0.0
    p0Denom = 0.0
    p1Denom = 0.0
    
    for i in range(numTrainDocs):
        #统计属于侮辱类的条件概率
        if trainCategory[i] ==1:
            p1Num += trainMatrix[i]
            p1Denom +=sum(trainMatrix[i])
        #统计属于非侮辱类的条件概率
        else:
            p0Num += trainMatrix[i]
            p0Denom += sum(trainMatrix[i])
            
    #相除
    p1Vect = p1Num/p1Denom
    p0Vect = p0Num/p0Denom
    #返回属于侮辱类的条件概率
    return p0Vect,p1Vect,pAbusive
if __name__=='__main__':
    postingList,classVec = loadDataSet()
    
    myVocabList = createVocabList(postingList)
    print('myVocabList',myVocabList)
    
    trainMat = []
    for postinDoc in postingList:
        trainMat.append(setOfWords2Vec(myVocabList,postinDoc))
        
    p0V,p1V,pAb = trainNB0(trainMat,classVec)
    print('p0V:\n',p0V)
    print("p1V:\n",p1V)
    print('classVec:\n',classVec)
    print('pAb:\n',pAb)
```

    myVocabList ['ate', 'not', 'help', 'my', 'stupid', 'licks', 'food', 'him', 'I', 'stop', 'posting', 'so', 'flea', 'dog', 'how', 'love', 'worthless', 'maybe', 'steak', 'garbage', 'quit', 'take', 'dalmation', 'please', 'to', 'problems', 'buying', 'has', 'park', 'mr', 'cute', 'is']
    p0V:
     [0.04166667 0.         0.04166667 0.125      0.         0.04166667
     0.         0.08333333 0.04166667 0.04166667 0.         0.04166667
     0.04166667 0.04166667 0.04166667 0.04166667 0.         0.
     0.04166667 0.         0.         0.         0.04166667 0.04166667
     0.04166667 0.04166667 0.         0.04166667 0.         0.04166667
     0.04166667 0.04166667]
    p1V:
     [0.         0.05263158 0.         0.         0.15789474 0.
     0.05263158 0.05263158 0.         0.05263158 0.05263158 0.
     0.         0.10526316 0.         0.         0.10526316 0.05263158
     0.         0.05263158 0.05263158 0.05263158 0.         0.
     0.05263158 0.         0.05263158 0.         0.05263158 0.
     0.         0.        ]
    classVec:
     [0, 1, 0, 1, 0, 1]
    pAb:
     0.5


p0V存放的是每个单词属于类别0，也就是非侮辱类词汇的概率。  
p1V存放的是每个单词属于类别1，也就是侮辱类词汇的概率。

已经训练好分类器，接着用分类器进行分类


```python
# -*- coding:UTF-8 -*-
import numpy as np
from functools import reduce
"""
函数说明：创建实验样本

Parameters：
        None
        
Returns：
        postingList - 实验样本切分的词条
        classVec - 类别标签向量
        
Modify：
        2020-04-23
"""
def loadDataSet():
    #切分词条
    postingList = [['my','dog','has','flea','problems','help','please'],
                  ['maybe','not','take','him','to','dog','park','stupid'],
                  ['my','dalmation','is','so','cute','I','love','him'],
                  ['stop','posting','stupid','worthless','garbage'],
                  ['mr','licks','ate','my','steak','how','to','stop','him'],
                  ['quit','buying','worthless','dog','food','stupid']]
    # 类别标签向量，1代表侮辱性词汇，0代表不是
    classVec = [0,1,0,1,0,1]
    return postingList, classVec


"""
函数说明：增加vocabList词汇表，将inputSet向量化，向量的每个元素为1或0

Parameters：
        vocabList - createVocabList返回的列表
        inputSet - 切分的词条列表
        
Returns：
        returnVec - 文档向量，词集模型
"""
def setOfWords2Vec(vocabList,inputSet):
    #创建一个其中所含元素都为0的向量
    returnVec =[0]*len(vocabList)
    #遍历每个词条
    for word in inputSet:
        #如果词条存在于词汇表中，则置1
        if word in vocabList:
            returnVec[vocabList.index(word)] = 1
        else:
            print("the word:s% is not in my Vocabulary!" % word)
    #返回向量文档
    return returnVec


"""
函数说明：将切分的实验样本词条整理成不重复的词条列表，也就是词汇表

Parameters：
        dataSet - 整理的样本数据集
        
Returns：
        vocabSet - 返回不重复的词条列表，也即是词汇表
        
Modify：
        2020-04-23
"""
def createVocabList(dataSet):
    #创建一个空的不重复的列表
    vocabSet = set([])
    for document in dataSet:
        #取并集
        vocabSet = vocabSet | set(document)
    return list(vocabSet)


"""
函数说明：朴素贝叶斯分类器训练函数

Parameters：
        trainMatrix - 训练文档矩阵，即setOfWords2Vec返回的returnVec构成的矩阵
        trainCategory - 训练类别标签向量，即loadDataSet返回的classVec

Returns：
        p0Vect - 侮辱类的条件概率数组
        p1Vect - 非侮辱类的条件概率数组
        pAbusive - 文档属于侮辱类的概率
        
Modify：
        2020-04-25
"""
def trainNB0(trainMatrix, trainCategory):
    #计算训练的文档数目
    numTrainDocs = len(trainMatrix)
    #计算每篇文章的词条数（即单词表单词个数）
    numWords = len(trainMatrix[0])
    #文档属于侮辱类的概率
    pAbusive = sum(trainCategory)/float(numTrainDocs)
    #创建numpy.zeros数组
    p0Num = np.zeros(numWords)
    p1Num = np.zeros(numWords)
    #分母初始化为0.0
    p0Denom = 0.0
    p1Denom = 0.0
    
    for i in range(numTrainDocs):
        #统计属于侮辱类的条件概率
        if trainCategory[i] ==1:
            p1Num += trainMatrix[i]
            p1Denom +=sum(trainMatrix[i])
        #统计属于非侮辱类的条件概率
        else:
            p0Num += trainMatrix[i]
            p0Denom += sum(trainMatrix[i])
            
    #相除
    p1Vect = p1Num/p1Denom
    p0Vect = p0Num/p0Denom
    #返回属于侮辱类的条件概率
    return p0Vect,p1Vect,pAbusive

"""
函数说明：朴素贝叶斯分类器分类函数

Parameters：
        vec2Classifyaaa - 待分类的词条数组
        p0Vec - 侮辱类的条件概率数组
        p1Vec - 非侮辱类的条件概率数组
        pClass1 - 文档属于侮辱类的概率
        
Returns：
        0 - 属于非侮辱类
        1 - 属于侮辱类
        
Modify：
        2020-04-25
        
"""
def classifyNB(vec2Classifyaaa, p0Vec, p1Vec, pClass1):
    #对应元素相乘
    p1 = reduce(lambda x,y:x*y,vec2Classifyaaa*p1Vec)*pClass1
    p0 = reduce(lambda x,y:x*y,vec2Classifyaaa*p0Vec)*(1.0-pClass1)
    print('p0:',p0)
    print('p1:',p1)
    if p1>p0:
        return 1
    else:
        return 0
    

    
"""
函数说明： 测试朴素贝叶斯分类器

Parameters：
        None

Returns：
        None
        
Modify：
        2020-04-25
        
"""
def testingNB():
    #创建实验样本
    list0Posts, listClasses = loadDataSet()
    #创建词汇表
    myVocabList = createVocabList((list0Posts))
    
    trainMat = []
    for postinDoc in list0Posts:
        #将实验样本向量化
        trainMat.append(setOfWords2Vec(myVocabList,postinDoc))
        
    #训练朴素贝叶斯分类器
    p0V,p1V,pAb = trainNB0(np.array(trainMat),np.array(listClasses))
    #测试样本1
    testEntry = ['love','my','dalmation']
    #测试样本向量化
    thisDoc = np.array(setOfWords2Vec(myVocabList,testEntry))
    #执行分类并打印分类结果
    if classifyNB(thisDoc,p0V,p1V,pAb):
        print(testEntry,'属于侮辱类')
    #执行分类并打印分类结果
    else:
        print(testEntry,'属于非侮辱类')
    #测试样本2
    testEntry = ['stupid','garbage']
    
    #测试样本向量化
    thisDoc = np.array(setOfWords2Vec(myVocabList,testEntry))
    #执行分类并打印分类结果
    if classifyNB(thisDoc, p0V, p1V, pAb):
        print(testEntry,'属于侮辱类')
    #执行分类并打印分类结果
    else:
        print(testEntry,'属于非侮辱类')
        
if __name__=='__main__':
    testingNB()
```

    p0: 0.0
    p1: 0.0
    ['love', 'my', 'dalmation'] 属于非侮辱类
    p0: 0.0
    p1: 0.0
    ['stupid', 'garbage'] 属于非侮辱类


我们发现p1和p2的计算结果都是0，下面我们来深刻探讨这个问题。

## 3.3 总结

#### 朴素贝叶斯推断的一些优点

* 生成式模型，通过计算概率来进行分类，可以用来处理多分类问题。
* 对小规模的数据表现很好，适合多分类任务，适合增量式训练，算法也比较简单。

#### 朴素贝叶斯推断的一些缺点

* 对输入数据的表达形式很敏感。
* 由于朴素贝叶斯的“朴素”特点，所以会带来一些准确率上的损失。
* 需要计算先验概率，分类决策存在错误率。

## 3.4 朴素贝叶斯改进——拉普拉斯平滑

1. 零概率问题

造成原因：

利用贝叶斯分类器对文档进行分类时，要计算多个概率额度乘积以获得文档属于某个类别的概率，即计算p(w0|A)p(w1|A)p(w2|A),如果其中一个为0，则最后的结果也为0.

解决方法：

为了降低这种影响，可以将所有词的出现次数初始化为1，并将分母初始化为2，这种做法称为“拉普拉斯平滑”，也称为“加1平滑”，是比较常用的平滑方法，为了解决0概率问题。

2. 下溢出

造成原因：

是太多很小的数相乘，越乘越小，就造成了下溢出的问题。在相应小数位置进行四舍五入，计算结果可能就变成0了。

解决办法：  
对乘积结果取自然对数，通过求对数可以避免下溢出或者浮点数舍入导致的错误，同时，采用自然对数进行处理不会有任何损失。

因此可以对trainNB0函数进行修改：


```python
# -*- coding:UTF-8 -*-
import numpy as np
from functools import reduce
"""
函数说明：创建实验样本

Parameters：
        None
        
Returns：
        postingList - 实验样本切分的词条
        classVec - 类别标签向量
        
Modify：
        2020-04-23
"""
def loadDataSet():
    #切分词条
    postingList = [['my','dog','has','flea','problems','help','please'],
                  ['maybe','not','take','him','to','dog','park','stupid'],
                  ['my','dalmation','is','so','cute','I','love','him'],
                  ['stop','posting','stupid','worthless','garbage'],
                  ['mr','licks','ate','my','steak','how','to','stop','him'],
                  ['quit','buying','worthless','dog','food','stupid']]
    # 类别标签向量，1代表侮辱性词汇，0代表不是
    classVec = [0,1,0,1,0,1]
    return postingList, classVec


"""
函数说明：增加vocabList词汇表，将inputSet向量化，向量的每个元素为1或0

Parameters：
        vocabList - createVocabList返回的列表
        inputSet - 切分的词条列表
        
Returns：
        returnVec - 文档向量，词集模型
"""
def setOfWords2Vec(vocabList,inputSet):
    #创建一个其中所含元素都为0的向量
    returnVec =[0]*len(vocabList)
    #遍历每个词条
    for word in inputSet:
        #如果词条存在于词汇表中，则置1
        if word in vocabList:
            returnVec[vocabList.index(word)] = 1
        else:
            print("the word:s% is not in my Vocabulary!" % word)
    #返回向量文档
    return returnVec


"""
函数说明：将切分的实验样本词条整理成不重复的词条列表，也就是词汇表

Parameters：
        dataSet - 整理的样本数据集
        
Returns：
        vocabSet - 返回不重复的词条列表，也即是词汇表
        
Modify：
        2020-04-23
"""
def createVocabList(dataSet):
    #创建一个空的不重复的列表
    vocabSet = set([])
    for document in dataSet:
        #取并集
        vocabSet = vocabSet | set(document)
    return list(vocabSet)


"""
函数说明：朴素贝叶斯分类器训练函数

Parameters：
        trainMatrix - 训练文档矩阵，即setOfWords2Vec返回的returnVec构成的矩阵
        trainCategory - 训练类别标签向量，即loadDataSet返回的classVec

Returns：
        p0Vect - 侮辱类的条件概率数组
        p1Vect - 非侮辱类的条件概率数组
        pAbusive - 文档属于侮辱类的概率
        
Modify：
        2020-04-25
"""
def trainNB0(trainMatrix, trainCategory):
    #计算训练的文档数目
    numTrainDocs = len(trainMatrix)
    #计算每篇文章的词条数（即单词表单词个数）
    numWords = len(trainMatrix[0])
    #文档属于侮辱类的概率
    pAbusive = sum(trainCategory)/float(numTrainDocs)
    #创建numpy.zeros数组
    p0Num = np.ones(numWords)
    p1Num = np.ones(numWords)
    #分母初始化为2.0，拉普拉斯平滑
    p0Denom = 2.0
    p1Denom = 2.0
    
    for i in range(numTrainDocs):
        #统计属于侮辱类的条件概率
        if trainCategory[i] ==1:
            p1Num += trainMatrix[i]
            p1Denom +=sum(trainMatrix[i])
        #统计属于非侮辱类的条件概率
        else:
            p0Num += trainMatrix[i]
            p0Denom += sum(trainMatrix[i])
            
    #相除
    p1Vect = np.log(p1Num/p1Denom)
    p0Vect = np.log(p0Num/p0Denom)
    #返回属于侮辱类的条件概率
    return p0Vect,p1Vect,pAbusive

if __name__=='__main__':
    postingList, classVec = loadDataSet()
    
    myVocabList = createVocabList(postingList)
    print('myVocabList:\n',myVocabList)
    
    trainMat = []
    for postinDoc in postingList:
        trainMat.append(setOfWords2Vec(myVocabList,postinDoc))
        
    
    p0V,p1V,pAb = trainNB0(trainMat,classVec)
    print('p0V:\n',p0V)
    print('p1V:\n',p1V)
    print('classVec:\n',classVec)
    print('pAb:\n',pAb)
```

    myVocabList:
     ['ate', 'not', 'help', 'my', 'stupid', 'licks', 'food', 'him', 'I', 'stop', 'posting', 'so', 'flea', 'dog', 'how', 'love', 'worthless', 'maybe', 'steak', 'garbage', 'quit', 'take', 'dalmation', 'please', 'to', 'problems', 'buying', 'has', 'park', 'mr', 'cute', 'is']
    p0V:
     [-2.56494936 -3.25809654 -2.56494936 -1.87180218 -3.25809654 -2.56494936
     -3.25809654 -2.15948425 -2.56494936 -2.56494936 -3.25809654 -2.56494936
     -2.56494936 -2.56494936 -2.56494936 -2.56494936 -3.25809654 -3.25809654
     -2.56494936 -3.25809654 -3.25809654 -3.25809654 -2.56494936 -2.56494936
     -2.56494936 -2.56494936 -3.25809654 -2.56494936 -3.25809654 -2.56494936
     -2.56494936 -2.56494936]
    p1V:
     [-3.04452244 -2.35137526 -3.04452244 -3.04452244 -1.65822808 -3.04452244
     -2.35137526 -2.35137526 -3.04452244 -2.35137526 -2.35137526 -3.04452244
     -3.04452244 -1.94591015 -3.04452244 -3.04452244 -1.94591015 -2.35137526
     -3.04452244 -2.35137526 -2.35137526 -2.35137526 -3.04452244 -3.04452244
     -2.35137526 -3.04452244 -2.35137526 -3.04452244 -2.35137526 -3.04452244
     -3.04452244 -3.04452244]
    classVec:
     [0, 1, 0, 1, 0, 1]
    pAb:
     0.5


此时已经不存在零概率了。

对classifyNB函数进行修改：


```python
# -*- coding:UTF-8 -*-
import numpy as np
from functools import reduce
"""
函数说明：创建实验样本

Parameters：
        None
        
Returns：
        postingList - 实验样本切分的词条
        classVec - 类别标签向量
        
Modify：
        2020-04-23
"""
def loadDataSet():
    #切分词条
    postingList = [['my','dog','has','flea','problems','help','please'],
                  ['maybe','not','take','him','to','dog','park','stupid'],
                  ['my','dalmation','is','so','cute','I','love','him'],
                  ['stop','posting','stupid','worthless','garbage'],
                  ['mr','licks','ate','my','steak','how','to','stop','him'],
                  ['quit','buying','worthless','dog','food','stupid']]
    # 类别标签向量，1代表侮辱性词汇，0代表不是
    classVec = [0,1,0,1,0,1]
    return postingList, classVec


"""
函数说明：增加vocabList词汇表，将inputSet向量化，向量的每个元素为1或0

Parameters：
        vocabList - createVocabList返回的列表
        inputSet - 切分的词条列表
        
Returns：
        returnVec - 文档向量，词集模型
"""
def setOfWords2Vec(vocabList,inputSet):
    #创建一个其中所含元素都为0的向量
    returnVec =[0]*len(vocabList)
    #遍历每个词条
    for word in inputSet:
        #如果词条存在于词汇表中，则置1
        if word in vocabList:
            returnVec[vocabList.index(word)] = 1
        else:
            print("the word:s% is not in my Vocabulary!" % word)
    #返回向量文档
    return returnVec


"""
函数说明：将切分的实验样本词条整理成不重复的词条列表，也就是词汇表

Parameters：
        dataSet - 整理的样本数据集
        
Returns：
        vocabSet - 返回不重复的词条列表，也即是词汇表
        
Modify：
        2020-04-23
"""
def createVocabList(dataSet):
    #创建一个空的不重复的列表
    vocabSet = set([])
    for document in dataSet:
        #取并集
        vocabSet = vocabSet | set(document)
    return list(vocabSet)


"""
函数说明：朴素贝叶斯分类器训练函数

Parameters：
        trainMatrix - 训练文档矩阵，即setOfWords2Vec返回的returnVec构成的矩阵
        trainCategory - 训练类别标签向量，即loadDataSet返回的classVec

Returns：
        p0Vect - 侮辱类的条件概率数组
        p1Vect - 非侮辱类的条件概率数组
        pAbusive - 文档属于侮辱类的概率
        
Modify：
        2020-04-25
"""
def trainNB0(trainMatrix, trainCategory):
    #计算训练的文档数目
    numTrainDocs = len(trainMatrix)
    #计算每篇文章的词条数（即单词表单词个数）
    numWords = len(trainMatrix[0])
    #文档属于侮辱类的概率
    pAbusive = sum(trainCategory)/float(numTrainDocs)
    #创建numpy.zeros数组
    p0Num = np.ones(numWords)
    p1Num = np.ones(numWords)
    #分母初始化为2.0，拉普拉斯平滑
    p0Denom = 2.0
    p1Denom = 2.0
    
    for i in range(numTrainDocs):
        #统计属于侮辱类的条件概率
        if trainCategory[i] ==1:
            p1Num += trainMatrix[i]
            p1Denom +=sum(trainMatrix[i])
        #统计属于非侮辱类的条件概率
        else:
            p0Num += trainMatrix[i]
            p0Denom += sum(trainMatrix[i])
            
    #相除
    p1Vect = np.log(p1Num/p1Denom)
    p0Vect = np.log(p0Num/p0Denom)
    #返回属于侮辱类的条件概率
    return p0Vect,p1Vect,pAbusive



"""
函数说明：朴素贝叶斯分类器分类函数

Parameters：
        vec2Classifyaaa - 待分类的词条数组
        p0Vec - 侮辱类的条件概率数组
        p1Vec - 非侮辱类的条件概率数组
        pClass1 - 文档属于侮辱类的概率
        
Returns：
        0 - 属于非侮辱类
        1 - 属于侮辱类
        
Modify：
        2020-04-25
        
"""
def classifyNB(vec2Classifyaaa, p0Vec, p1Vec, pClass1):
    #对应元素相乘，logA*B=logA+logB，所以要加上np.log(pClass1)
    p1 = sum(vec2Classifyaaa*p1Vec)+np.log(pClass1)
    p0 = sum(vec2Classifyaaa*p0Vec)+np.log(1.0-pClass1)
   
    if p1>p0:
        return 1
    else:
        return 0

"""
函数说明： 测试朴素贝叶斯分类器

Parameters：
        None

Returns：
        None
        
Modify：
        2020-04-25
        
"""
def testingNB():
    #创建实验样本
    list0Posts, listClasses = loadDataSet()
    #创建词汇表
    myVocabList = createVocabList((list0Posts))
    
    trainMat = []
    for postinDoc in list0Posts:
        #将实验样本向量化
        trainMat.append(setOfWords2Vec(myVocabList,postinDoc))
        
    #训练朴素贝叶斯分类器
    p0V,p1V,pAb = trainNB0(np.array(trainMat),np.array(listClasses))
    #测试样本1
    testEntry = ['love','my','dalmation']
    #测试样本向量化
    thisDoc = np.array(setOfWords2Vec(myVocabList,testEntry))
    #执行分类并打印分类结果
    if classifyNB(thisDoc,p0V,p1V,pAb):
        print(testEntry,'属于侮辱类')
    #执行分类并打印分类结果
    else:
        print(testEntry,'属于非侮辱类')
    #测试样本2
    testEntry = ['stupid','garbage']
    
    #测试样本向量化
    thisDoc = np.array(setOfWords2Vec(myVocabList,testEntry))
    #执行分类并打印分类结果
    if classifyNB(thisDoc, p0V, p1V, pAb):
        print(testEntry,'属于侮辱类')
    #执行分类并打印分类结果
    else:
        print(testEntry,'属于非侮辱类')
        
if __name__=='__main__':
    testingNB()
```

    ['love', 'my', 'dalmation'] 属于非侮辱类
    ['stupid', 'garbage'] 属于侮辱类


## 3.5 朴素贝叶斯——过滤垃圾邮件

朴素贝叶斯的最著名的应用——电子邮件垃圾过滤

步骤：

* 收集数据：提供文本文件。
* 准备数据：将文本文件解析成词条向量。
* 分析数据：检查词条确保解析的正确性。
* 训练算法：使用我们之前建立的trainNB0()函数
* 测试算法：使用classifyNB()，并构建一个新的测试函数来计算文档的错误率。
* 使用算法：构建一个完整的程序对一组文档进行分类，将错分的文档输出到屏幕上。

### 3.5.1 收集数据

有两个文件夹，ham和spam，spam文件下的txt文件为垃圾邮件。

### 3.5.2 准备数据

对于英文文本，我们可以以非字母、非数字作为符号进行切分，使用split函数即可。编写代码如下：


```python
# -*- coding:UTF-8 -*-
import re

"""
函数说明：接收一个大字符串并将其解析为字符串列表

Parameters：
        None
        
Returns：
        None
        
Modify：
        2020-04-25
"""
def textParse(bigString):
    #将特殊符号作为切分标志进行字符串的切分，即非字母、非数字
    listOfTokens = re.split(r"\W",bigString)
    #除了单个字母，例如大写的l，其他单词变成小写
    return [tok.lower() for tok in listOfTokens if len(tok)>2]




"""
函数说明：将切分的实验样本词条整理成不重复的词条列表，也就是词汇表

Parameters：
        dataSet - 整理的样本数据集
        
Returns：
        vocabSet - 返回不重复的词条列表，也就是词汇表
        
Modify：
        2020-04-25
"""
def createVocabList(dataSet):
    #创建一个空的不重复的列表
    vocabSet = set([])
    for document in dataSet:
        #取并集
        vocabSet = vocabSet | set(document)
    return list(vocabSet)

if __name__=='__main__':
    docList = []
    classList = []
    #遍历25个txt文件
    for i in range(1,26):
        #读取每个垃圾邮件，并将字符串转换成字符串列表
        wordList = textParse(open('spam/%d.txt' % i,'r').read())
        docList.append(wordList)
        #标记垃圾邮件，1表示垃圾邮件
        classList.append(1)
        #读取每个非垃圾邮件，并将字符串转换为字符串列表
        wordList = textParse(open('ham/%d.txt' % i,'r').read())
        docList.append(wordList)
        #标记非垃圾邮件，0表示非垃圾邮件
        classList.append(0)
        #创建词汇表，不重复
    vocabList = createVocabList(docList)
    print(vocabList)
```

    ['safe', 'cartier', 'free', 'said', 'changes', 'intenseorgasns', 'check', 'got', 'since', 'cost', 'prototype', 'rent', 'you', 'differ', 'yourpenis', 'success', 'service', 'watches', 'bad', 'received', 'get', 'place', 'possible', '430', 'level', '180', 'wholesale', 'herbal', 'call', 'selected', 'program', 'income', 'withoutprescription', '750', 'party', 'incredib1e', 'please', 'cca', 'prices', 'advocate', 'book', 'softwares', 'add', 'ambiem', 'retirement', '50092', 'launch', '195', 'pretty', 'answer', 'featured', 'much', 'financial', 'concise', '562', 'louis', 'home', 'province', 'mail', '325', 'arolexbvlgari', 'lunch', 'order', 'sent', 'used', 'giants', 'your', 'ideas', 'major', '0nline', 'how', 'runs', '10mg', 'color', 'finder', 'told', 'quantitative', 'supporting', 'address', 'troy', 'wednesday', 'core', 'works', 'same', 'game', 'let', '138', 'them', 'job', 'pricing', 'knew', 'accepted', 'access', 'jpgs', 'cold', 'signed', 'has', 'focus', 'hours', 'articles', 'cards', 'father', 'jay', 'significantly', 'but', 'issues', 'strategy', 'borders', 'longer', 'forward', 'brained', 'development', 'only', 'wilson', 'methylmorphine', 'upload', 'ofejacu1ate', 'because', 'storedetailview_98', 'com', 'bike', 'reply', 'insights', 'important', 'encourage', 'couple', 'came', 'members', '570', 'those', 'mandatory', 'stepp', 'cs5', 'speedpost', 'magazine', 'pharmacy', 'warranty', 'coast', 'behind', 'note', 'carlo', 'cheers', 'tour', 'buy', 'drugs', 'had', 'jqplot', 'genuine', 'knocking', 'today', 'hamm', 'being', 'money', '322', 'window', 'roofer', 'class', 'arvind', 'art', 'sky', 'link', 'tesla', 'email', 'interesting', '130', '129', 'www', 'enough', 'derivatives', 'series', '625', 'don', 'shipment', 'courier', 'jar', 'fractal', 'length', 'hommies', 'announcement', 'also', 'cuda', 'heard', 'grounds', 'release', '203', 'germany', 'requested', 'china', 'two', '120', 'off', 'want', 'fans', 'web', 'too', 'inform', 'reliever', 'yesterday', 'edit', 'groups', 'risk', 'experience', '292', 'chinese', 'style', 'foaming', 'bin', 'cannot', 'management', 'working', 'done', 'designed', 'gpu', 'chance', 'treat', 'enabled', 'this', 'mandarin', 'york', 'huge', 'who', 'new', 'inside', 'they', 'products', 'viagranoprescription', '100m', 'winter', 'credit', 'via', '291', 'earn', 'creative', 'station', 'source', 'needed', 'inches', 'have', 'will', 'jewerly', 'often', 'see', 'femaleviagra', 'hydrocodone', 'once', 'hotels', '25mg', 'rock', 'dhl', 'died', 'hangzhou', 'doggy', 'thought', '66343', 'jquery', 'leaves', 'when', 'going', 'mom', 'great', 'connection', 'dusty', 'tickets', '513', 'both', 'quality', 'plus', 'one', 'talked', 'yay', 'least', 'for', 'top', 'hold', 'phone', 'network', 'thanks', 'some', 'individual', 'thirumalai', 'stuff', 'superb', 'vivek', '156', 'store', 'permanantly', 'survive', 'ready', 'brandviagra', 'oris', 'naturalpenisenhancement', 'high', 'the', 'percocet', 'methods', 'automatically', 'enjoy', 'logged', 'specifications', 'focusing', 'endorsed', 'millions', 'accept', 'train', 'while', 'share', 'expo', 'increase', 'bathroom', 'shape', 'fermi', 'extended', 'hotel', 'cats', 'pill', 'john', 'name', 'required', 'creation', 'gain', 'just', 'pain', 'far', 'vuitton', 'thousand', 'writing', 'gains', 'horn', 'life', '492', 'model', 'low', 'benoit', 'python', 'automatic', 'certified', 'turd', '2007', '14th', 'than', 'everything', 'http', 'such', 'page', 'not', 'need', 'does', '2011', 'perhaps', 'museum', 'mailing', '366', 'site', 'supplement', 'another', 'ups', 'february', 'about', 'doors', 'lists', 'ryan', 'per', 'features', 'rude', 'explosive', 'there', 'scenic', 'car', 'sure', 'kerry', 'copy', 'code', 'specifically', 'functionalities', 'create', 'file', 'placed', 'below', 'computer', 'follow', 'famous', 'ems', 'fbi', 'biggerpenis', 'commented', 'bettererections', 'narcotic', 'uses', 'whybrew', 'could', 'fine', 'blue', 'bags', 'gas', 'trusted', 'worldwide', 'recieve', 'inspired', 'canadian', 'design', 'hermes', 'moderately', 'watchesstore', 'support', 'take', 'phentermin', 'comment', '119', 'strategic', 'instead', 'view', 'doing', 'jose', 'definitely', 'saw', 'expertise', 'ones', 'amex', 'ultimate', 'over', 'generation', 'number', 'starting', 'competitive', 'come', 'welcome', 'express', 'google', 'noprescription', 'made', 'plugin', 'trip', 'most', 'held', 'reservation', 'safest', 'what', 'natural', 'holiday', 'adobe', 'opportunity', 'help', '219', 'opioid', 'pick', 'ferguson', 'sliding', 'work', 'buyviagra', 'linkedin', 'message', 'delivery', '225', 'save', 'eugene', 'pages', 'online', 'can', 'exhibit', 'freeviagra', 'discussions', 'faster', 'dior', 'any', '15mg', 'spaying', 'ma1eenhancement', 'brands', 'close', 'day', 'approved', 'away', 'tokyo', 'tent', 'way', 'transformed', 'find', 'information', 'plane', 'must', 'guaranteeed', 'like', 'fda', 'website', 'know', 'questions', 'things', 'and', 'analgesic', 'private', 'grow', 'meet', 'programming', 'having', 'good', 'contact', 'betterejacu1ation', 'that', 'based', 'learn', 'scifinance', 'price', 'example', 'cat', 'all', 'now', 'listed', 'acrobat', 'discount', 'tabs', 'tool', 'incoming', 'decision', 'wasn', 'files', 'fedex', 'photoshop', 'year', 'rain', 'watson', 'door', 'generates', 'with', 'net', 'yeah', 'owner', 'glimpse', 'went', 'riding', 'moneyback', 'reputable', '2010', 'attaching', '200', 'suggest', 'pictures', 'wallets', '588', 'guy', '1924', 'days', 'windows', 'mandelbrot', 'pills', 'serial', 'team', 'vicodin', 'well', 'item', 'time', 'full', 'group', 'zolpidem', 'parallel', 'thailand', 'try', '5mg', '90563', 'was', 'drunk', 'others', 'status', 'school', 'forum', 'thread', 'oem', 'pls', 'butt', 'thing', 'moderate', 'titles', 'town', 'proven', 'mba', 'julius', 'thank', 'then', 'nature', '300x', 'mathematician', 'sites', 'cheap', 'regards', 'invitation', 'are', 'sophisticated', 'assigning', 'his', 'nvidia', 'right', 'out', 'office', 'gucci', 'harderecetions', 'hope', 'capabilities', 'here', 'amazing', 'food', 'aged', 'assistance', 'fast', 'wilmott', 'easily', 'back', 'looking', 'ordercializviagra', 'control', 'docs', 'use', 'peter', 'brand', 'thickness', 'zach', 'favorite', 'changing', 'update', 'keep', 'volume', 'location', 'think', 'improving', 'care', 'dozen', 'would', 'includes', 'discreet', 'customized', 'sounds', 'running', 'chapter', 'codeine', 'notification', '86152', 'severepain', 'inconvenience', 'been', 'may', 'shipping', 'tiffany', '385', 'located', 'where', 'modelling', 'wrote', '50mg', '100', 'using', 'computing', 'might', '30mg', 'through', 'visa', 'hello', 'sorry', 'girl', 'latest', 'download', 'effective', 'prepared', 'items', 'storage', 'lined', 'doctor', 'past', '396', 'each', 'either', 'pavilion', 'pro', 'should', 'night', 'bargains', 'from', 'monte', 'jocelyn', 'microsoft', 'of_penisen1argement', 'professional', 'business', 'mathematics', 'these', '100mg', 'haloney', 'finance', '174623', '199', 'more', 'october', 'fundamental', 'experts', 'approach']


根据词汇表，可以将每个文本向量化，此处将数据集分为训练集和测试集，使用交叉验证的方式测试朴素贝叶斯分类器的准确性，代码如下：


```python
# -*- coding:UTF-8 -*-
import numpy as np
import re
import random

"""
函数说明：创建实验样本

Parameters：
        None
        
Returns：
        postingList - 实验样本切分的词条
        classVec - 类别标签向量
        
Modify：
        2020-04-23
"""
def loadDataSet():
    #切分词条
    postingList = [['my','dog','has','flea','problems','help','please'],
                  ['maybe','not','take','him','to','dog','park','stupid'],
                  ['my','dalmation','is','so','cute','I','love','him'],
                  ['stop','posting','stupid','worthless','garbage'],
                  ['mr','licks','ate','my','steak','how','to','stop','him'],
                  ['quit','buying','worthless','dog','food','stupid']]
    # 类别标签向量，1代表侮辱性词汇，0代表不是
    classVec = [0,1,0,1,0,1]
    return postingList, classVec


"""
函数说明：增加vocabList词汇表，将inputSet向量化，向量的每个元素为1或0

Parameters：
        vocabList - createVocabList返回的列表
        inputSet - 切分的词条列表
        
Returns：
        returnVec - 文档向量，词集模型
"""
def setOfWords2Vec(vocabList,inputSet):
    #创建一个其中所含元素都为0的向量
    returnVec =[0]*len(vocabList)
    #遍历每个词条
    for word in inputSet:
        #如果词条存在于词汇表中，则置1
        if word in vocabList:
            returnVec[vocabList.index(word)] = 1
        else:
            print("the word:s% is not in my Vocabulary!" % word)
    #返回向量文档
    return returnVec


"""
函数说明：将切分的实验样本词条整理成不重复的词条列表，也就是词汇表

Parameters：
        dataSet - 整理的样本数据集
        
Returns：
        vocabSet - 返回不重复的词条列表，也即是词汇表
        
Modify：
        2020-04-23
"""
def createVocabList(dataSet):
    #创建一个空的不重复的列表
    vocabSet = set([])
    for document in dataSet:
        #取并集
        vocabSet = vocabSet | set(document)
    return list(vocabSet)



"""
函数说明：根据vocabList词汇表，构建词袋模型

Parameters：
        vocabList - createVocabList返回的列表
        inputSet - 切分的词条列表
        
Returns：
        returnVec - 文档向量，词袋模型
        
Modify ：
        2020-04-26
"""
def bagOfWordsVecMN(vocabList,inputSet):
    #创建一个其中所含元素都为0的向量
    returnVec = [0]*len(vocabList)
    #遍历每个词条
    for word in inputSet:
        #如果词条存在于词汇表中，则计数加1
        if word in vocabList:
            returnVec[vocabList.index(word)] +=1
    #返回词袋模型
    return returnVec



"""
函数说明：朴素贝叶斯分类器训练函数

Parameters：
        trainMatrix - 训练文档矩阵，即setOfWords2Vec返回的returnVec构成的矩阵
        trainCategory - 训练类别标签向量，即loadDataSet返回的classVec

Returns：
        p0Vect - 侮辱类的条件概率数组
        p1Vect - 非侮辱类的条件概率数组
        pAbusive - 文档属于侮辱类的概率
        
Modify：
        2020-04-25
"""
def trainNB0(trainMatrix, trainCategory):
    #计算训练的文档数目
    numTrainDocs = len(trainMatrix)
    #计算每篇文章的词条数（即单词表单词个数）
    numWords = len(trainMatrix[0])
    #文档属于侮辱类的概率
    pAbusive = sum(trainCategory)/float(numTrainDocs)
    #创建numpy.zeros数组
    p0Num = np.ones(numWords)
    p1Num = np.ones(numWords)
    #分母初始化为2.0，拉普拉斯平滑
    p0Denom = 2.0
    p1Denom = 2.0
    
    for i in range(numTrainDocs):
        #统计属于侮辱类的条件概率
        if trainCategory[i] ==1:
            p1Num += trainMatrix[i]
            p1Denom +=sum(trainMatrix[i])
        #统计属于非侮辱类的条件概率
        else:
            p0Num += trainMatrix[i]
            p0Denom += sum(trainMatrix[i])
            
    #相除
    p1Vect = np.log(p1Num/p1Denom)
    p0Vect = np.log(p0Num/p0Denom)
    #返回属于侮辱类的条件概率
    return p0Vect,p1Vect,pAbusive



"""
函数说明：朴素贝叶斯分类器分类函数

Parameters：
        vec2Classifyaaa - 待分类的词条数组
        p0Vec - 侮辱类的条件概率数组
        p1Vec - 非侮辱类的条件概率数组
        pClass1 - 文档属于侮辱类的概率
        
Returns：
        0 - 属于非侮辱类
        1 - 属于侮辱类
        
Modify：
        2020-04-25
        
"""
def classifyNB(vec2Classifyaaa, p0Vec, p1Vec, pClass1):
    #对应元素相乘，logA*B=logA+logB，所以要加上np.log(pClass1)
    p1 = sum(vec2Classifyaaa*p1Vec)+np.log(pClass1)
    p0 = sum(vec2Classifyaaa*p0Vec)+np.log(1.0-pClass1)
   
    if p1>p0:
        return 1
    else:
        return 0
    
    
"""
函数说明：接收一个大字符串并将其解析为字符串列表

Parameters：
        None
        
Returns：
        None
        
Modify：
        2020-04-26
"""
def textParse(bigString):
    #将特殊符号作为切分标志进行字符串的切分，即非字母、非数字
    listOfTokens = re.split(r"\W",bigString)
    #除了单个字母，例如大写的l，其他单词变成小写
    return [tok.lower() for tok in listOfTokens if len(tok)>2]



"""
函数说明： 测试朴素贝叶斯分类器

Parameters：
        None

Returns：
        None
        
Modify：
        2020-04-25
        
"""
def spamTest():
    docList = []
    classList = []
    fullText = []
    #遍历25个txt文件
    for i in range(1,26):
        #读取每个垃圾邮件，并将字符串转换成字符串列表
        wordList = textParse(open('spam/%d.txt' % i, 'r').read())
        docList.append(wordList)
        fullText.append(wordList)
        #标记垃圾邮件，1表示垃圾邮件
        classList.append(1)
        #读取每个非垃圾邮件，并将字符串转化成字符串列表
        wordList = textParse(open('ham/%d.txt' % i, 'r').read())
        docList.append(wordList)
        fullText.append(wordList)
        #标记非垃圾邮件，0表示垃圾邮件
        classList.append(0)
    #创建词汇表，不重复
    vocabList = createVocabList(docList)
    #创建存储训练集的索引值的列表和测试集的索引值的列表
    trainingSet = list(range(50))
    testSet = []
    #从50个邮件中随机选出40个作为训练集，10个作为测试
    for i in range(10):
        #随机选取索引值
        randIndex = int(random.uniform(0,len(trainingSet)))
        #添加测试机的索引值
        testSet.append(trainingSet[randIndex])
        #在训练集列表中删除添加到测试集的索引值
        del (trainingSet[randIndex])
    #创建训练集矩阵和训练集类别标签向量
    trainMat = []
    trainClasses = []
    #遍历训练集
    for docIndex in trainingSet:
        #将生成的词集模型添加到训练集矩阵中
        trainMat.append(setOfWords2Vec(vocabList,docList[docIndex]))
        #将类别添加到训练集类别标签向量中
        trainClasses.append(classList[docIndex])
    #训练朴素贝叶斯模型
    p0V, p1V, pSpam = trainNB0(np.array(trainMat), np.array(trainClasses))
    #错误分类计数
    errorCount = 0
    #遍历测试集
    for docIndex in testSet:
        #测试集的词集模型
        wordVector = setOfWords2Vec(vocabList, docList[docIndex])
        #如果分类错误，错误计数加1
        if classifyNB(np.array(wordVector), p0V,p1V,pSpam) != classList[docIndex]:
            errorCount +=1
            print("分类错误的测试集：",docList[docIndex])
    print("错误率：%.2f%%" % (float(errorCount) / len(testSet) * 100))
    

if __name__=='__main__':
    spamTest()
```

    分类错误的测试集： ['home', 'based', 'business', 'opportunity', 'knocking', 'your', 'door', 'don', 'rude', 'and', 'let', 'this', 'chance', 'you', 'can', 'earn', 'great', 'income', 'and', 'find', 'your', 'financial', 'life', 'transformed', 'learn', 'more', 'here', 'your', 'success', 'work', 'from', 'home', 'finder', 'experts']
    错误率：10.00%


函数spamTest()会输出10封随机选择的电子邮件上的分类错误率。既然这些电子邮件是随机选择的，所以每次的输出结果可能有差别。

## 3.6 朴素贝叶斯——新浪新闻分类(sklearn)

### 中文语句却分

可以直接使用第三方分词组件，即jieba库。

数据集分类结果如下：

![image](https://github.com/Zhang-Sun/My-first-proj/blob/机器学习/朴素贝叶斯算法实战/朴素贝叶斯图像与数据/classList.png)

切分中文语句代码：


```python
# -*- coding:UTF-8 -*-
"""
函数说明：切分中文语句

Parameters：
        doler_path - 数据集文件夹路径
        
Returns：
        None
    
Modify：
        2020-04-26
"""
import os
import jieba

def TextProcessing(folder_path):
    #查看folder_path下的文件
    folder_list = os.listdir(folder_path)
    #训练集
    data_list = []
    class_list = []
    
    #遍历每一个子文件夹
    for folder in folder_list:
        if folder != '.DS_Store':
            #根据子文件夹，生成新的路径
            new_folder_path = os.path.join(folder_path,folder)
            #存放子文件夹下的txt文件列表
            files = os.listdir
        
        j = 1
        #遍历每一个txt文件
        for file in files:
            #每类txt样本数最多100个
            if j>100:
                break
            #打开txt文件
            with open(os.path.join(new_folder_path,file),'r',encoding='utf-8') as f:
                raw = f.read()
            #精简模式，返回一个可迭代的generator
            word_cut = jieba.cut(raw,cut_all = False)
            #generator转换成list
            word_list = list(word_cut)
        
            data_list.append(word_list)
            class_list.append(folder)
            j+=1
        print(data_list)
        print(class_list)
    
if __me__=='__main__':
    文本预处理
    #训练集存放地址
    folder_path = 'Sample'
    TextProcessing(folder_path)
```

    [['\u3000', '\u3000', '本报讯', ' ', '（', '记者', '李英辉', '）', '已经', '退出', '北京', '市场', '两年', '多', '的', '车贷险', '业务', '重现', '市场', '。', '记者', '昨天', '获悉', '，', '安邦', '财险', '将', '在', '本月', '启动', '车贷险', '业务', '。', '\n', '\u3000', '\u3000', '购买', '车贷险', '后', '，', '一旦', '贷款人', '不能', '还', '贷款', '，', '保险公司', '要', '负责', '赔偿', '银行贷款', '。', '该', '险种', '面市', '后', '，', '曾', '极大', '地', '促进', '了', '银行', '车贷', '业务', '的', '发展', '。', '由于', '车贷险', '的', '赔付率', '竟然', '超过', '100%', '，', '保险公司', '不堪重负', '，', '两年', '前', '全面', '退出', '市场', '。', '\n', '\u3000', '\u3000', '新', '车贷险', '做', '了', '重大', '调整', '。', '原来', '只要', '被保险人', '逾期', '还款', '，', '保险公司', '就', '须代其', '还款', '。', '新', '条款', '中', '，', '保险公司', '的', '履约', '责任', '由', '第一位', '降到', '第二位', '，', '如', '出现', '被保险人',
    
### 3.6.2 文本特征选择

我们将所有文本分成测试集和训练集，并对训练集中的所有单词进行词频统计，并按降序排序。也就是将出现次数最多的词语在前，出现次数最少的词语在后进行排序。编写代码如下：


```python
# -*- coding:UTF-8 -*-

import os
import jieba
import random 
"""
函数说明：切分中文语句

Parameters：
        doler_path - 数据集文件夹路径
        
Returns：
        None
    
Modify：
        2020-04-26
"""


def TextProcessing(folder_path,test_size=0.2):
    #查看folder_path下的文件
    folder_list = os.listdir(folder_path)
    #训练集
    data_list = []
    class_list = []
    
    #遍历每一个子文件夹
    for folder in folder_list:
        if folder != '.DS_Store':
            #根据子文件夹，生成新的路径
            new_folder_path = os.path.join(folder_path,folder)
            #存放子文件夹下的txt文件列表
            files = os.listdir(new_folder_path)
        
        j = 1
        #遍历每一个txt文件
        for file in files:
            #每类txt样本数最多100个
            if j>100:
                break
            #打开txt文件
            with open(os.path.join(new_folder_path,file),'r',encoding='utf-8') as f:
                raw = f.read()
            #精简模式，返回一个可迭代的generator
            word_cut = jieba.cut(raw,cut_all = False)
            #generator转换成list
            word_list = list(word_cut)
        
            data_list.append(word_list)
            class_list.append(folder)
            j+=1
    #zip压缩合并，将数据与标签对应压缩
    data_class_list = list(zip(data_list, class_list))
    #将data_class_list乱序
    random.shuffle(data_class_list)
    #训练集与测试集切分的索引值
    index = int(len(data_class_list)*test_size)+1
    #训练集
    train_list = data_class_list[index:]
    #测试集
    test_list = data_class_list[:index]
    #训练集解压缩
    train_data_list, train_class_list = zip(*train_list)
    #测试集解压缩
    test_data_list, test_class_list = zip(*test_list)
    #统计训练集词频
    all_words_dict = {}
    for word_list in train_data_list:
        for word in word_list:
            if word in all_words_dict.keys():
                all_words_dict[word]+=1
            else:
                all_words_dict[word] = 1
    #根据键值倒序排列
    all_words_tuple_list = sorted(all_words_dict.items(), key=lambda f:f[1], reverse=True)
    #解压缩
    all_words_list,all_words_num = zip(*all_words_tuple_list)
    #转换成列表
    all_words_list = list(all_words_list)
    return all_words_list,train_data_list,test_data_list,train_class_list,test_class_list

if __name__=='__main__':
    #文本预处理，训练集存放的地址
    folder_path = 'Sample'
    all_words_list,train_data_list,test_data_list,train_class_list,test_class_list = TextProcessing(folder_path,test_size=0.2)
```

输出的all_words_list就是将所有训练集的切分结果按照词频降序排列构成的单词集合，前面包含了很多标点符号，如‘是’，‘的’，‘在’等字，还有数字。所以要将这些去掉。

去掉的规则：去掉高频词，至于去掉多少，则根据高频词个数和最终检测率的关系来确定。

如何去掉，可以使用已经整理好的stopwords_cn.txt文本来去除高频词，不作为分类的特征，首先去除100个，代码如下：


```python
# -*- coding:UTF-8 -*-

import os
import jieba
import random 
"""
函数说明：切分中文语句

Parameters：
        doler_path - 数据集文件夹路径
        
Returns：
        None
    
Modify：
        2020-04-26
"""


def TextProcessing(folder_path,test_size=0.2):
    #查看folder_path下的文件
    folder_list = os.listdir(folder_path)
    #训练集
    data_list = []
    class_list = []
    
    #遍历每一个子文件夹
    for folder in folder_list:
        if folder != '.DS_Store':
            #根据子文件夹，生成新的路径
            new_folder_path = os.path.join(folder_path,folder)
            #存放子文件夹下的txt文件列表
            files = os.listdir(new_folder_path)
        
        j = 1
        #遍历每一个txt文件
        for file in files:
            #每类txt样本数最多100个
            if j>100:
                break
            #打开txt文件
            with open(os.path.join(new_folder_path,file),'r',encoding='utf-8') as f:
                raw = f.read()
            #精简模式，返回一个可迭代的generator
            word_cut = jieba.cut(raw,cut_all = False)
            #generator转换成list
            word_list = list(word_cut)
        
            data_list.append(word_list)
            class_list.append(folder)
            j+=1
    #zip压缩合并，将数据与标签对应压缩
    data_class_list = list(zip(data_list, class_list))
    #将data_class_list乱序
    random.shuffle(data_class_list)
    #训练集与测试集切分的索引值
    index = int(len(data_class_list)*test_size)+1
    #训练集
    train_list = data_class_list[index:]
    #测试集
    test_list = data_class_list[:index]
    #训练集解压缩
    train_data_list, train_class_list = zip(*train_list)
    #测试集解压缩
    test_data_list, test_class_list = zip(*test_list)
    #统计训练集词频
    all_words_dict = {}
    for word_list in train_data_list:
        for word in word_list:
            if word in all_words_dict.keys():
                all_words_dict[word]+=1
            else:
                all_words_dict[word] = 1
    #根据键值倒序排列
    all_words_tuple_list = sorted(all_words_dict.items(), key=lambda f:f[1], reverse=True)
    #解压缩
    all_words_list,all_words_num = zip(*all_words_tuple_list)
    #转换成列表
    all_words_list = list(all_words_list)
    return all_words_list,train_data_list,test_data_list,train_class_list,test_class_list


"""
函数说明：读取文件中的内容并去重

Parameters：
        words_file - 文件路径

Returns：
        word_set - 读取内容的set 集合
        
Modify：
        2020-04-29
"""
def MakeWordSet(words_file):
    #创建set集合
    words_set = set()
    #打开文件
    with open(words_file,'r',encoding='utf-8') as f:
        #一行一行读取
        for line in f.readlines():
            #去会车
            word = line.strip()
            #有文本，则添加到word_set
            if len(word)>0:
                words_set.add(word)
    #返回处理结果
    return words_set




"""
函数说明：文本特征读取

Parameters：
        all_words_list - 训练集所有的文本列表
        deleteN - 删除词频最高的deleteN个词
        stopwords_set -指定的结束语
        
Returns：
        feature_words - 特征集
        
Modify：
        2020 - 04 -29
        
"""
def words_dict(all_words_list,deleteN,stopWords_set = set()):
    #特征列表
    feature_words = []
    n = 1 
    for t in range(deleteN,len(all_words_list),1):
        #feature_words的纬度为1000
        if n>1000:
            break
        #如果这个词不是数字，且不是指定的结束语，并且单词长度大于1小于5，那么这个词就可以作为特征词
        if not all_words_list[t].isdigit() and all_words_list[t] not in stopWords_set and 1<len(all_words_list[t]):
            feature_words.append(all_words_list[t])
        n+=1
    return feature_words


if __name__=='__main__':
    #文本处理，训练集存放的地址
    folder_path = 'Sample'
    all_words_list, train_data_list, test_data_list, train_class_list, test_class_list = TextProcessing(folder_path, test_size = 0.2)
    
    #生成stopwords_set
    stopwords_file = 'stopwords_cn.txt'
    stopwords_set = MakeWordSet(stopwords_file)
    
    feature_words = words_dict(all_words_list, 100, stopwords_set)
    print(feature_words)
    

```

    ['发展', '目前', '黄金周', '复习', '时间', '主要', '可能', '建设', '选择', '五一', '很多', '上海', '仿制', '认为', '问题', '需要', '增长', '管理', '一定', '基础', '学习', '学校', '项目', '设计', '网络', '员工', '能力', '辅导班', '接待', '万人次', '专业', '记者', '用户', '词汇', '通过', '时候', '重要', '一家', '计划', '美国', '亿美元', '期间', '平台', '毕业生', '提高', '训练', '部队', '今年', '影响', '收入', '表示', 'VS', '填报', '开始', '发现', '专家', '比较', '经济', '坦克', '顾客', '全国', '分析', '不能', '交易', '科学', '一直', '国家', '各种', '国内', '要求', '必须', '服务', '活动', '方式', '相关', '参加', '提供', '考试', '考虑', '系统', '英语', '客户', '考古', '关国光', '重点', 'MBA', '老师', '考研', '目标', '方面', '实验室', '装备', '最大', '业务', '希望', '行业', '专利', '全面', '不同', '注意', '不会', '成功', '进入', '达到', '产品', '休闲', '人数', '耿大勇', '晋升', '两个', '一种', '人才', '往往', '文化', '指挥', '超过', '包括', '销售', '电话', '睡眠', '第一', '数学', '部分', '努力', '阅读', '现在', '左右', '全军', '参与', '工程', '景区', '香港', '面对', '第三方', '代表', '数独', '决定', '这种', '关系', '之间', '建立', '掌握', '完成', '一下', '发展观', '历史', '正在', '技术', '组织', '获得', '价值', '吸引', '愿意', '消费者', '写作', '招聘', '投入', '领导', '知识', '利用', '是否', '赔偿', '未来', '战场', '社会', '情况', '网上支付', '应该', '商业银行', '这家', '牛奶', '预期', '药厂', '最佳', '教育', '得到', '过程', '容易', '相当', '机会', '喜欢', '了解', '录取', '我军', '新型', '同比', '图库', '比赛', '同事', '数据', '南京', '游戏', '知识点', '概念', '建议', '连续', '关键', '理解', '准备', '知道', '最后', '出现', '基本', '稳定', '地方', '显示', '数量', '原因', '网上', '军队', '实现', '一批', '研究', '信息化', '完全', '医院', '越来越', '协议', '标志', '介绍', '备考', '语法', '东南亚', '新浪', '安妮', '汪力', '分期付款', '知名', '方法', '思路', '应用', '每个', '院校', '环境', '电子', '演练', '医疗', '亿元', '因素', '带来', '调查', '经理', '负责', '这是', '著名', '数字', '营养', '辽宁队', '本场', '告诉', '沈阳市', '提升', '特点', '条件', '内容', '支持', '更加', '以下', '满足', '大量', '竞争', '国际', '看到', '旅游者', '学员', '非常', '使用', '俄罗斯', '认证', '利苑', '有限公司', '起来', '职业', '来源', '不要', '传统', '不断', '加强', '办法', '信息', '展示', '资源', '景点', '规则', '句子', '预计', '蓝军', '治疗', '账户', '不少', '媒体', '手机', '今天', '免息', '一页', '标准', '水平', '进一步', '其实', '找到', '综合', '产生', '复试', '坚持', '万元', '实施', '改革', '主动', '导弹', '形成', '过去', '积极', '市民', '发布', '我国', '冰山', '作用', '统计', '旅行社', '安排', '我省', '上司', '业绩', '上市', '增加', 'AK', '东莞', '振保', '一次', '提出', '真实', '感觉', '共同', '规模', '优秀', '合作', '经验', '整个', '方向', '展开', '解题', '公式', '最终', '一年', '本科', '领域', '去年', '保证', '一起', '每天', '指出', '取得', '推出', '官兵', '一场', '创造', '刚刚', '诊断', '具有', '生产', '广东', '帮助', '结束', '消费', '明显', '本报记者', '购买', '增幅', '文物', '遗址', '迅速', '之前', '最好', '能够', '教材', '公布', '战斗', '职位', '商机', '状态', '翻译', '食物', '失眠', '客场', '保险公司', '酒家', '密码', '批次', 'H股', '股东', '导演', '形象', '举办', '高级', '培养', '促进', '题型', '平时', '相互', '联系', '核心', '自然', '几个', '涉及', '事情', '依然', '大学', '提前', '力量', '三个', '很快', '这一', '开展', '单位', '先后', '世界', '两年', '战争', '操作', '模拟', '几年', '几乎', '采取', '机制', '空间', '原则', '金融', '运动', '承诺', '内部', '不足', '广告', '联结', '真正', '发挥', '同期', '整合', '效果', '表现', '日本', '报道', '评选', '听课', '之后', '文章', '详细', '邮票', '协会', '孩子', '失去', '团队', 'gt', '去年同期', '大学生', '医药', '邓珉', '老板', '狂妄', '工具', '从事', '快速', '全球', '吸收', '车贷险', '补报', '一位', '突破', '类似', '参看', '矩阵', '相对', '好好', '关注', '充分', '生活', '很大', '之一', '直接', '诉讼', '日电', '国防', '有效', '主题', '出台', '动力', '为主', '保障', '海上', '各地', '乡村', '出游', '总监', '一半', '信任', '董事长', '即将', '接近', '似乎', '风格', '大多数', '心理', '利润', '采用', '显著', '上年', '泰国', '研究所', '本书', 'CEO', '集团', '再次', '投诉', '城市', '轻松', '表明', '下属', '避免', '功能', '营销', '最近', '红军', '人体', '主场', '范文', '降价', '辽足', '马林', '唐尧东', '场位', '敏华', '初盘', '牙膏', '可选报', '埃弗顿', '吸烟', '戒烟', '也许', '语言', '更大', '评估', '培训', '双方', '这部分', '练习', '同学', '做到', '一点', '成立', '一般', '概率', '性质', '记忆', '突出', '解决', '考题', '特别', '一样', '风险', '十分', '鼓励', '无疑', '经营', '阶段', '贯彻', '纳入', '结合', '具体', '举行', '发出', '结果', '增强', '行动', '活力', '细节', '程度', '地区', '出境游', '商户', '央行', '投资', '人口', '定位', '感受', '意思', '独特', '觉得', '看来', '根基', '一项', '收益', '清晰', '荷兰', '充足', '学生', '精读', '多年', '不再', '公民', '东北亚', '人士', '苏宁', '电器', '高度', '投资者', '本报', '装甲团', '连队', '第一次', '患者', '女兵', '经常', '商业', '设立', '行政', '哪里', '药方', '第二', '跳槽', '推广', '简历', '猎头', '市值', '制药', '竞标', '意味着', '身高', '生长', '足彩', '联赛', '佛罗伦萨', '切沃', '俱乐部', '销售额', '泰华', '兵器', '红玫瑰', '白玫瑰', '张爱玲', '指导', '新加坡', '回到', '重新', '更为', '各级', '政府', '重视', '模式', '总结', '简单', '大部分', '同样', '比例', '相当于', '说明', '线性代数', '熟悉', '理论', '函数', '考前', '学科', '辅导书', '变化', '面临', '必要', '胜利', '成绩', '优势', '良好', '调剂', '晚上', '二外', '理想', '招生', '搜索', '本报讯', '价格', '出版', '昨天', '责任', '宣布', '继续', '军事', '推动', '持续', '发生', '总部', '干部', '制定', '首次', '运用', '保持', '一体化', '效应', '饮食', '集中', '采购', '基地', '正式', '启动', '之中', '开通', '热点', '目的地', '咨询', '报告', '补充', '很难', '联想', '服务业', '强大', '现有', '存在', '钱币', '毕竟', '安全', '水面', 'com', '分为', '门票', '广州', '十大', '意识', '引起', '意义', '参考书', '题目', '解析', '分钟', '看看', '难度', '季泽', '旅游圈', '仍然', '离开', '增值', '实力', '困难', '命令', '精神', '股骨头', '大型', '生意']


从结果可以看出来，我们已经过滤了哪些没用的词组，这些feature_words就是我们最终选出用于新闻分类的特征，随后就可以根据特征词将文本向量化，然后用于训练朴素贝叶斯分类器。

## 3.7 使用sklearn构建朴素贝叶斯分类器

在scikit-learn中，一共有三个朴素贝叶斯分类的算法。分别是：GaussianNB，MultinomialNB和BernoulliNB。

* GaussianNB就是先验为高斯分布的朴素贝叶斯；
* MultinomialNB就是先验为多项式分布的朴素贝叶斯；
* BernoulliNB就是先验为伯努利分布的朴素贝叶斯。

前面讲的先验概率模型就是先验概率为多项式分布的朴素贝叶斯。以下是sklearn中naive_bayes中的方法：

* naive_bayes.BernoulliNB([alpha, binarize, ... ])  伯努利分布模型的朴素贝叶斯分类器。
* naive_bayes.GaussianNB([priors, var_smoothing])  高斯分布模型的朴素贝叶斯分类器。
* naive_bayes.MultinomialNB([alpha, ... ])  多项式模型的朴素贝叶斯分类器。
* naive_bayes.ComplementNB([alpha, fit_prior, ... ])  The Complement Naive Bayes classifier described in Rennie et al.

对于新闻的分类属于多分类问题，可以使用MultinomialNB来完成，假设特征的先验概率为多项式分布。


```python
class sklearn.naive_bayes.MultinomialNB(alpha=1.0, fit_prior=True, class_prior=None)
```

* alpha：浮点型可选参数，默认为1.0，其实就是添加拉普拉斯平滑，如果这个参数为0，就是不添加拉普拉斯平滑。
* fit_prior：布尔型可选参数，默认值是True。表示是否要考虑先验概率，如果是false，则所有的样本类别输出都有相同的类别先验概率。否则自己可以用第三个参数class_prior输入先验概率，或者不输入第三个参数让MultinomialNB自己从训练样本中计算先验概率，此时的先验概率为P(Y=Ck)=mk/m。其中m为训练样本总数量，mk为输出为第k类别的训练样集本数，
* class_prior：可选参数，默认为None。

提供的方法：

* fit(X,y[,sample_weight])  通过X，y拟合朴素贝叶斯分类器
* get_params([deep])  为估计器获取参数
* partial_fit(X,y[,classes, sample-weight])  一批样本的增量拟合
* predict(X)  对测试向量X的数组执行分类。
* predict_log_proba(X)  返回对测试向量的对数概率估计
* predict_proba(X) 返回测试向量的概率估计
* score(X, y[, sample_weight])  返回给出测试数据和标签的平均精度
* set_params(**params)  设置分估计器的参数。

**fit**：一般的拟合。  
**partial_fit**：一般用在训练集数据很大，一次不能全部载入内存的时候，这个时候可以把训练集分成若干等份，重复调用该方法来一步步学习训练集。  
**predict**：常用的预测方法，直接给出测试集的预测类别输出。  
**predict_log_proba**：预测出的各个类别对数概率里最大值对应的类别，也就是predict方法得到类别。  
**predict_proba**：它会给出测试集样本在各个类别上的预测的概率，预测出的各个类别概率里的最大值对应的类别，也就是predict方法得到的类别。


确定要去掉deleteN个高频词的个数与最终检测准确率的关系，确定deleteN的取值。


```python
%matplotlib inline
# -*- coding:UTF-8 -*-

import os
import jieba
import random 
from sklearn.naive_bayes import MultinomialNB
import matplotlib.pyplot as plt
"""
函数说明：切分中文语句

Parameters：
        doler_path - 数据集文件夹路径
        
Returns：
        None
    
Modify：
        2020-04-26
"""


def TextProcessing(folder_path,test_size=0.2):
    #查看folder_path下的文件
    folder_list = os.listdir(folder_path)
    #训练集
    data_list = []
    class_list = []
    
    #遍历每一个子文件夹
    for folder in folder_list:
        if folder != '.DS_Store':
            #根据子文件夹，生成新的路径
            new_folder_path = os.path.join(folder_path,folder)
            #存放子文件夹下的txt文件列表
            files = os.listdir(new_folder_path)
        
        j = 1
        #遍历每一个txt文件
        for file in files:
            #每类txt样本数最多100个
            if j>100:
                break
            #打开txt文件
            with open(os.path.join(new_folder_path,file),'r',encoding='utf-8') as f:
                raw = f.read()
            #精简模式，返回一个可迭代的generator
            word_cut = jieba.cut(raw,cut_all = False)
            #generator转换成list
            word_list = list(word_cut)
        
            data_list.append(word_list)
            class_list.append(folder)
            j+=1
    #zip压缩合并，将数据与标签对应压缩
    data_class_list = list(zip(data_list, class_list))
    #将data_class_list乱序
    random.shuffle(data_class_list)
    #训练集与测试集切分的索引值
    index = int(len(data_class_list)*test_size)+1
    #训练集
    train_list = data_class_list[index:]
    #测试集
    test_list = data_class_list[:index]
    #训练集解压缩
    train_data_list, train_class_list = zip(*train_list)
    #测试集解压缩
    test_data_list, test_class_list = zip(*test_list)
    #统计训练集词频
    all_words_dict = {}
    for word_list in train_data_list:
        for word in word_list:
            if word in all_words_dict.keys():
                all_words_dict[word]+=1
            else:
                all_words_dict[word] = 1
    #根据键值倒序排列
    all_words_tuple_list = sorted(all_words_dict.items(), key=lambda f:f[1], reverse=True)
    #解压缩
    all_words_list,all_words_num = zip(*all_words_tuple_list)
    #转换成列表
    all_words_list = list(all_words_list)
    return all_words_list,train_data_list,test_data_list,train_class_list,test_class_list


"""
函数说明：读取文件中的内容并去重

Parameters：
        words_file - 文件路径

Returns：
        word_set - 读取内容的set 集合
        
Modify：
        2020-04-29
"""
def MakeWordSet(words_file):
    #创建set集合
    words_set = set()
    #打开文件
    with open(words_file,'r',encoding='utf-8') as f:
        #一行一行读取
        for line in f.readlines():
            #去会车
            word = line.strip()
            #有文本，则添加到word_set
            if len(word)>0:
                words_set.add(word)
    #返回处理结果
    return words_set

def TextFeatures(train_data_list, test_data_list, feature_words):
    # 出现在特征集中，则置1
    def text_features(text, feature_words):
        text_words = set(text)
        features = [1 if word in text_words else 0 for word in feature_words]
        return features
    train_feature_list = [text_features(text, feature_words) for text in train_data_list]
    test_feature_list = [text_features(text, feature_words) for text in test_data_list]
    # 返回结果
    return train_feature_list, test_feature_list



"""
函数说明：文本特征读取

Parameters：
        all_words_list - 训练集所有的文本列表
        deleteN - 删除词频最高的deleteN个词
        stopwords_set -指定的结束语
        
Returns：
        feature_words - 特征集
        
Modify：
        2020 - 04 -29
        
"""
def words_dict(all_words_list,deleteN,stopWords_set = set()):
    #特征列表
    feature_words = []
    n = 1 
    for t in range(deleteN,len(all_words_list),1):
        #feature_words的纬度为1000
        if n>1000:
            break
        #如果这个词不是数字，且不是指定的结束语，并且单词长度大于1小于5，那么这个词就可以作为特征词
        if not all_words_list[t].isdigit() and all_words_list[t] not in stopWords_set and 1<len(all_words_list[t]):
            feature_words.append(all_words_list[t])
        n+=1
    return feature_words


"""
函数说明：新闻分类器

parameters：
    train_feature_list - 训练集向量化的特征文本
    test_feature_list - 测试集向量化的特征文本
    train_class_list - 训练集分类标签
    test_class_list - 测试集分类标签
Returns:
    test_accuracy - 分类器精度
Modify:
    2020-04-29
"""
def TextClassifier(train_feature_list,test_feature_list,train_class_list,test_class_list):
    classifier=MultinomialNB().fit(train_feature_list,train_class_list)
    test_accuracy=classifier.score(test_feature_list,test_class_list)
    return test_accuracy

if __name__=='__main__':
    #文本预处理，训练集存放的地址
    folder_path='Sample'
    all_words_list, train_data_list, test_data_list, train_class_list, \
         test_class_list=TextProcessing(folder_path,test_size=0.2)

    #生成stopwords_set
    stopwords_file='stopwords_cn.txt'
    stopwords_set=MakeWordSet(stopwords_file)

    test_accuracy_list=[]
    deleteNs=range(0,1000,20)
    for deleteN in deleteNs:
        feature_words=words_dict(all_words_list,deleteN,stopwords_set)
        train_feature_list,test_feature_list=TextFeatures(train_data_list,
                                                        test_data_list,feature_words)
        test_accuracy=TextClassifier(train_feature_list,test_feature_list,
                                     train_class_list,test_class_list)
        test_accuracy_list.append(test_accuracy)


    plt.figure()
    plt.plot(deleteNs,test_accuracy_list)
    plt.title('Relationship of deleteNs and test_accuracy')
    plt.xlabel('deleteNs')
    plt.ylabel('test_accuracy')
    plt.show()
```


![png](output_20_0.png)


## 3.7 总结

* 在训练朴素贝叶斯分类器之前，要处理好训练集，文本的清洗还是有很多需要学习的东西。
* 根据提取的分类特征将文本向量化，然后训练朴素贝叶斯分类器。
* 去高频词汇数量的不同，对结果也是有影响的。
* 拉普拉斯平滑对于改善朴素贝叶斯分类器的分类效果有着积极的作用。


```python

```
